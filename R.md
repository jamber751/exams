# Оглавление

| Билет | Вопрос 1 | Вопрос 2 |
|:---:|:---|:---|
| **1** | [Задачи анализа данных](#1-задачи-анализа-данных) | [Логистическая регрессия](#2-логистическая-регрессия-для-решения-задач-прогнозирования-и-классификации-пример) |
| **2** | [Аналитика больших данных (Big Data)](#1-аналитика-больших-данных-big-data) | [Метод главных компонент (PCA)](#2-метод-главных-компонент-pca-пример) |
| **3** | [Метод работы с пропущенными данными](#1-метод-работы-с-пропущенными-данными) | [Временные ряды](#2-временные-ряды-пример) |
| **4** | [Линейный дискриминантный анализ](#1-линейный-дискриминантный-анализ) | [Линейная регрессия](#2-линейная-регрессия-для-решения-задач-прогнозирования-и-классификации-пример) |
| **5** | [Решение СЛАУ с использованием R](#1-решение-слау-с-использованием-r-примеры) | [Множественная линейная регрессия](#2-множественная-линейная-регрессия-пример) |
| **6** | [Работа с матрицами и таблицами данных в R](#1-работа-с-матрицами-и-таблицами-данных-в-r) | [Дерево принятия решений](#2-дерево-принятия-решений-пример) |
| **7** | [KNN](#1-knn-пример) | [Сравнение зависимых групп](#2-сравнение-зависимых-групп-пример) |
| **8** | [Средства интеллектуального анализа данных](#1-средства-интеллектуального-анализа-данных) | [Сравнение зависимых групп](#2-сравнение-зависимых-групп-пример-1) |
| **9** | [Классификация задач анализа данных](#1-классификация-задач-анализа-данных) | [Нейронные сети и анализ данных](#2-нейронные-сети-и-анализ-данных-пример) |
| **10** | [Средства визуализации результатов анализа данных](#1-средства-визуализации-результатов-анализа-данных) | [Сравнение двух независимых групп](#2-сравнение-двух-независимых-групп-пример) |
| **11** | [Линейная регрессия и классификация](#1-линейная-регрессия-и-классификация-пример) | [Методы визуализации с использованием R](#2-методы-визуализации-с-использованием-r) |
| **12** | [Множественная линейная регрессия](#1-множественная-линейная-регрессия-пример) | [Рекомендательные системы](#2-рекомендательные-системы-пример) |
| **13** | [Метод ближайших соседей (KNN)](#1-метод-ближайших-соседей-knn-пример) | [Работа с массивами в R](#2-работа-с-массивами-в-r) |
| **14** | [Ассоциативные правила](#1-ассоциативные-правила-пример) | [Проверка статистических гипотез в среде R](#2-проверка-статистических-гипотез-в-среде-r) |
| **15** | [Алгоритм k-means](#1-алгоритм-k-means-пример) | [Рекомендательные системы и анализ данных](#2-рекомендательные-системы-и-анализ-данных) |
| **16** | [Алгоритм kNN](#1-алгоритм-knn-пример) | [Дисперсионный анализ](#2-дисперсионный-анализ-пример) |
| **17** | [Кластерный анализ](#1-кластерный-анализ-пример) | [Проверка статистических гипотез в среде R](#2-проверка-статистических-гипотез-в-среде-r-1) |
| **18** | [Требования к данным, подготовка данных](#1-требования-к-данным-подготовка-данных-пример) | [Нейронные сети и анализ данных](#2-нейронные-сети-и-анализ-данных-пример-1) |
| **19** | [Основные понятия интеллектуального анализа данных](#1-основные-понятия-интеллектуального-анализа-данных) | [Нейронные сети и анализ данных](#2-нейронные-сети-и-анализ-данных-пример-2) |
| **20** | [Алгоритм PCA](#1-алгоритм-pca-пример) | [Очистка данных](#2-очистка-данных-пример) |
| **21** | [Алгоритм k-means](#1-алгоритм-k-means-пример-1) | [Процесс анализа данных](#2-процесс-анализа-данных) |
| **22** | [Алгоритм kNN](#1-алгоритм-knn-пример-1) | [Подготовка данных. Работа с пропусками в R](#2-подготовка-данных-работа-с-пропусками-в-r) |
| **23** | [Задачи аналитики данных](#1-задачи-аналитики-данных-пример) | [Алгоритм кластеризации](#2-алгоритм-кластеризации-пример) |
| **24** | [Построение графиков в среде R](#1-построение-графиков-в-среде-r) | [Линейная регрессия](#2-линейная-регрессия-пример) |
| **25** | [Средства визуализации для аналитики данных](#1-средства-визуализации-для-аналитики-данных) | [Нейронные сети и анализ данных](#2-нейронные-сети-и-анализ-данных-пример-3) |

---

## 1\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Задачи анализа данных

Задачи анализа данных (Data Mining) можно разделить на:

1.  **Классификация (Classification):** Отнесение объекта к одному из предопределенных классов (например, прогнозирование оттока клиента, определение спама).
2.  **Регрессия (Regression):** Прогнозирование непрерывного числового значения (например, цены дома, температуры, спроса).
3.  **Кластеризация (Clustering):** Группировка схожих объектов в кластеры без заранее заданных меток (например, сегментация рынка).
4.  **Ассоциативные правила (Association Rule Mining):** Поиск закономерностей вида "если А, то В" (например, анализ корзины покупок).
5.  **Снижение размерности (Dimensionality Reduction):** Уменьшение числа переменных при сохранении основной информации (например, PCA).

### 2\. Логистическая регрессия, для решения задач прогнозирования и классификации, пример

#### Теория

  * **Назначение:** Используется для задач **классификации**, когда зависимая переменная является **бинарной** (0/1, да/нет).
  * **Принцип:** Моделирует вероятность принадлежности объекта к определенному классу, используя логистическую функцию (сигмоиду): $P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \dots)}}$.
  * **Функция в R:** `glm()` с параметром `family = binomial`.

#### Пример кода (R)

Прогнозирование приема на работу (`admit`) по оценкам (`gpa`) и опыту (`gre`).

```r
# Установка и загрузка пакета
# install.packages("AER")
library(AER)

# Загрузка данных (пример: данные о приеме в вуз)
data(Affairs)
data <- data.frame(admit = ifelse(Affairs$affairs > 0, 1, 0), 
                   rating = Affairs$rating, 
                   age = Affairs$age)

# Построение модели
model_logit <- glm(admit ~ rating + age, 
                   data = data, 
                   family = binomial)

# Краткий вывод модели
summary(model_logit)

# Прогнозирование вероятностей
probabilities <- predict(model_logit, type = "response")

# Преобразование вероятностей в классы (классификация)
predictions <- ifelse(probabilities > 0.5, 1, 0)
```

-----

## 2\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Аналитика больших данных (Big Data)

#### Теория

  * **Определение:** Анализ данных, которые характеризуются тремя V: **Volume** (большой объем), **Velocity** (высокая скорость генерации), **Variety** (разнообразие форматов). Часто добавляют четвертое V: **Veracity** (достоверность).
  * **Цель:** Извлечение ценных знаний, скрытых закономерностей и инсайтов из массивных и сложных наборов данных.
  * **Инструменты/Среды:** Для работы с Big Data в R часто используются интеграции с экосистемой Hadoop/Spark (например, пакеты `sparklyr`, `RHadoop`), а также специализированные пакеты для эффективной работы с памятью и параллельными вычислениями.

### 2\. Метод главных компонент (PCA), пример

#### Теория

  * **Назначение:** **Снижение размерности** данных и **визуализация**. Преобразует набор, возможно, коррелированных переменных в набор линейно-некоррелированных переменных, называемых **главными компонентами (Principal Components)**.
  * **Принцип:** Находит направления (компоненты) с наибольшей дисперсией в данных. Первая компонента объясняет наибольшую дисперсию, вторая — наибольшую оставшуюся дисперсию, и так далее.
  * **Функция в R:** `prcomp()`.

#### Пример кода (R)

```r
# Загрузка встроенного набора данных (например, Ирисы Фишера)
data(iris)

# Применение PCA только к числовым столбцам
iris.pca <- prcomp(iris[, 1:4], 
                   center = TRUE, 
                   scale. = TRUE) # Центрирование и масштабирование (важно для PCA)

# Вывод результатов
summary(iris.pca)

# Доля дисперсии, объясненная каждой компонентой
print(iris.pca$sdev^2 / sum(iris.pca$sdev^2))

# Визуализация результатов (биоплот)
# install.packages("factoextra")
library(factoextra)
fviz_pca_ind(iris.pca, 
             geom.ind = "point", # Только точки
             col.ind = iris$Species, # Раскраска по виду
             legend.title = "Вид")
```

-----

## 3\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Метод работы с пропущенными данными

#### Теория

Пропущенные данные (NA - Not Available) могут сильно исказить результаты анализа. Основные подходы:

1.  **Удаление:**
      * **Строгое удаление (Listwise Deletion):** Удаление всех строк, содержащих хотя бы одно пропущенное значение. Просто, но может привести к потере большого объема данных. Функция: `na.omit()`.
2.  **Заполнение (Imputation):** Замена пропущенных значений:
      * **Константой:** Нулем, или специфическим значением.
      * **Средним/Медианой/Модой:** Замена средним (для нормального распределения) или медианой (для скошенного) по столбцу.
      * **Горячая/Холодная колода (Hot/Cold Deck):** Случайная замена значением из других похожих записей.
      * **Моделирование (Model-based Imputation):** Использование алгоритмов (например, регрессии, KNN, MICE) для прогнозирования пропущенных значений. Пакет: `mice`.

#### Пример кода (R)

Использование медианы для заполнения пропусков.

```r
# Создание примера данных с пропусками
data <- data.frame(A = c(1, 2, NA, 4, 5), 
                   B = c(NA, 10, 20, 30, 40))
print(data)

# Проверка количества пропусков
sapply(data, function(x) sum(is.na(x)))

# 1. Удаление строк с пропусками
data_clean_omit <- na.omit(data)

# 2. Заполнение пропусков медианой
median_A <- median(data$A, na.rm = TRUE)
data$A[is.na(data$A)] <- median_A

median_B <- median(data$B, na.rm = TRUE)
data$B[is.na(data$B)] <- median_B

print(data)
```

### 2\. Временные ряды, пример

#### Теория

  * **Определение:** Последовательность точек данных, измеренных в последовательные моменты времени.
  * **Компоненты:**
      * **Тренд (Trend):** Долгосрочное направление движения.
      * **Сезонность (Seasonality):** Повторяющиеся колебания через фиксированные интервалы (например, ежедневные, месячные).
      * **Цикл (Cycle):** Колебания без фиксированной периодичности.
      * **Шум (Noise/Irregularity):** Случайные, непредсказуемые изменения.
  * **Модели:** ARIMA (AutoRegressive Integrated Moving Average), ETS (Error, Trend, Seasonality) для экспоненциального сглаживания.
  * **Пакет в R:** `forecast`.

#### Пример кода (R)

Прогнозирование с помощью ARIMA.

```r
# Установка и загрузка пакета
# install.packages("forecast")
library(forecast)

# Загрузка встроенного набора данных о пассажирах авиалиний (monthly)
data(AirPassengers) 
ts_data <- AirPassengers

# Визуализация ряда
plot(ts_data, main = "Пассажиры авиалиний")

# Построение модели ARIMA с автоматическим выбором параметров
model_arima <- auto.arima(ts_data)

# Прогноз на 24 месяца вперед
forecast_arima <- forecast(model_arima, h = 24)

# Визуализация прогноза
plot(forecast_arima)
```

-----

## 4\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Линейный дискриминантный анализ

#### Теория

  * **Назначение:** **Классификация** и **снижение размерности**. Используется для разделения двух или более классов объектов путем проецирования данных на оси, которые максимизируют межклассовую дисперсию и минимизируют внутриклассовую дисперсию.
  * **Предположения:** Нормальное распределение данных внутри каждого класса и **равенство ковариационных матриц** (гомоскедастичность) для всех классов.
  * **Функция в R:** `lda()` из пакета `MASS`.

#### Пример кода (R)

```r
# Установка и загрузка пакета
library(MASS)
data(iris)

# Построение модели LDA для классификации видов ирисов
model_lda <- lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
                 data = iris)

# Вывод результатов (показывает коэффициенты линейных дискриминантов)
print(model_lda)

# Прогнозирование
predictions <- predict(model_lda, iris)$class

# Матрица неточностей
table(predictions, iris$Species)
```

### 2\. Линейная регрессия для решения задач прогнозирования и классификации, пример

#### Теория

  * **Назначение:** **Прогнозирование** непрерывных переменных (регрессия).
  * **Принцип:** Моделирует линейную зависимость между зависимой переменной ($Y$) и одной или несколькими независимыми переменными ($X$): $Y = \beta_0 + \beta_1 X_1 + \dots + \varepsilon$.
  * **Функция в R:** `lm()` (Linear Model).
  * **Использование для классификации:** **Не рекомендуется** для бинарной классификации напрямую, так как прогнозируемые значения могут выходить за пределы $[0, 1]$. Для классификации лучше использовать **Логистическую регрессию**.

#### Пример кода (R)

Прогнозирование цены дома (`medv`) по количеству комнат (`rm`) (на примере данных Boston).

```r
# Установка и загрузка пакета
# install.packages("MASS")
library(MASS)
data(Boston)

# Построение простой линейной регрессии
model_lm <- lm(medv ~ rm, data = Boston)

# Краткий вывод модели
summary(model_lm)

# Визуализация
plot(Boston$rm, Boston$medv, 
     main = "Линейная регрессия: Цена vs. Комнаты",
     xlab = "Среднее количество комнат (rm)",
     ylab = "Медианная цена (medv)")
# Добавление линии регрессии
abline(model_lm, col = "red")
```

-----

## 5\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Решение СЛАУ с использованием R, примеры

#### Теория

Система линейных алгебраических уравнений (СЛАУ) вида $A \cdot x = b$ может быть решена в R с использованием матричных операций.

  * $A$ — матрица коэффициентов.
  * $x$ — вектор неизвестных.
  * $b$ — вектор свободных членов.
  * **Решение:** $x = A^{-1} \cdot b$.

#### Функции в R

  * `solve(A, b)`: Находит $x$ путем вычисления $A^{-1}b$ (предпочтительный, более стабильный метод).
  * `solve(A)`: Вычисляет обратную матрицу $A^{-1}$.

#### Пример кода (R)

Решение системы:

$$
\begin{cases} 
2x + 3y = 8 \\ 
x - 2y = -3 
\end{cases}
$$

```r
# Матрица коэффициентов A
A <- matrix(c(2, 1, 3, -2), 
            nrow = 2, 
            byrow = TRUE)

# Вектор свободных членов b
b <- c(8, -3)

# 1. Решение с помощью solve(A, b) (предпочтительно)
x_sol <- solve(A, b)
print("Решение x, y:")
print(x_sol) # Ответ: x=1, y=2

# 2. Решение с использованием обратной матрицы A_inv (только для справки)
A_inv <- solve(A)
x_inv <- A_inv %*% b # %*% - оператор матричного умножения
print("Решение с помощью обратной матрицы:")
print(x_inv)
```

### 2\. Множественная линейная регрессия, пример

#### Теория

  * **Назначение:** Прогнозирование непрерывной зависимой переменной ($Y$) с использованием **двух или более** независимых переменных ($X_1, X_2, \dots, X_k$).
  * **Модель:** $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k + \varepsilon$.
  * **Интерпретация:** $\beta_i$ показывает, насколько изменится $Y$ при увеличении $X_i$ на единицу, **при условии, что остальные предикторы остаются неизменными** (ceteris paribus).
  * **Функция в R:** `lm()`.

#### Пример кода (R)

Прогнозирование расхода топлива (`mpg`) по объему двигателя (`disp`) и мощности (`hp`).

```r
data(mtcars)

# Построение модели множественной линейной регрессии
model_multi_lm <- lm(mpg ~ disp + hp, data = mtcars)

# Краткий вывод модели
summary(model_multi_lm)

# Извлечение коэффициентов
coefficients <- coef(model_multi_lm)
print("Коэффициенты регрессии:")
print(coefficients)

# Пример прогнозирования
new_data <- data.frame(disp = 200, hp = 100)
prediction <- predict(model_multi_lm, newdata = new_data)
print("Прогноз mpg для новых данных:")
print(prediction)
```

-----

## 6\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Работа с матрицами и таблицами данных в R

#### Теория

  * **Матрица (Matrix):** Двумерный массив, все элементы которого **должны быть одного типа** (числа, строки, логические). Создается функцией `matrix()`.
  * **Таблица данных/Датафрейм (Data Frame):** Наиболее распространенная структура для данных. Это список векторов одинаковой длины, где **каждый столбец может иметь свой тип данных** (например, числовой, факторный, строковый). Создается функцией `data.frame()`.

#### Основные операции

| Операция | Матрица/Датафрейм | Код (Пример) |
| :--- | :--- | :--- |
| Создание | `matrix(data, nrow, ncol)` | `M <- matrix(1:9, 3, 3)` |
| Создание | `data.frame(col1=vec1, col2=vec2)` | `DF <- data.frame(A=1:3, B=c('x','y','z'))` |
| Просмотр | `head()`, `str()` | `head(DF)`, `str(DF)` |
| Выборка (строки, столбцы) | Индексация `[строки, столбцы]` | `DF[1, 2]`, `DF[ , "B"]` |
| Фильтрация (только датафрейм) | Логическая индексация | `DF[DF$A > 1, ]` |

#### Пример кода (R)

```r
# Создание матрицы
M <- matrix(1:6, 
            nrow = 2, 
            byrow = TRUE)
print("Матрица M:")
print(M)

# Создание датафрейма
DF <- data.frame(
  ID = 1:5,
  Score = c(85, 92, 78, 95, 88),
  Passed = c(TRUE, TRUE, FALSE, TRUE, TRUE)
)
print("Датафрейм DF:")
print(DF)

# Выборка: столбец Score
print("Столбец Score:")
print(DF$Score)

# Фильтрация: строки, где Passed = FALSE
print("Не сдавшие:")
print(DF[DF$Passed == FALSE, ])
```

### 2\. Дерево принятия решений, пример

#### Теория

  * **Назначение:** Используется для **классификации** и **регрессии**. Создает модель в виде дерева, где каждый **внутренний узел** представляет проверку на признак, **ветка** представляет результат проверки, а **листовой узел** представляет метку класса или прогнозируемое значение.
  * **Принцип:** Рекурсивное разделение данных на основе признаков, которые наилучшим образом уменьшают неопределенность (например, используя критерии **Джини** или **Энтропию/Информационный выигрыш**).
  * **Функция в R:** `rpart()` из пакета `rpart`.

#### Пример кода (R)

```r
# Установка и загрузка пакетов
# install.packages(c("rpart", "rpart.plot"))
library(rpart)
library(rpart.plot)
data(iris)

# Преобразование зависимой переменной в фактор
iris$Species <- as.factor(iris$Species)

# Построение дерева решений
model_tree <- rpart(Species ~ ., 
                    data = iris, 
                    method = "class") # "class" для классификации

# Визуализация дерева
rpart.plot(model_tree, 
           type = 4, 
           extra = 101, 
           main = "Дерево решений для Iris")

# Прогнозирование
predictions <- predict(model_tree, iris, type = "class")

# Оценка точности
conf_matrix <- table(predictions, iris$Species)
print("Матрица неточностей:")
print(conf_matrix)
```

-----

## 7\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. KNN, пример

#### Теория

  * **Назначение:** **Классификация** и **Регрессия**. Это ленивый (lazy) алгоритм, то есть не обучается явной модели.
  * **Принцип (Классификация):** Для нового объекта находит **K** ближайших соседей (на основе метрики расстояния, например, Евклидова). Класс нового объекта определяется **большинством голосов** среди его K соседей.
  * **Гиперпараметр K:** Выбор $K$ критичен. Малое $K$ чувствительно к шуму; большое $K$ размывает границы классов.
  * **Пакет в R:** `class` (функция `knn`).

#### Пример кода (R)

```r
# Установка и загрузка пакета
library(class)
data(iris)

# Подготовка данных: стандартизация и разделение на тренировочный/тестовый наборы
# Стандартизация (важно для KNN)
iris_scaled <- scale(iris[, 1:4])

# Разделение данных
set.seed(42)
train_indices <- sample(1:nrow(iris), 100)
train_data <- iris_scaled[train_indices, ]
test_data <- iris_scaled[-train_indices, ]
train_labels <- iris$Species[train_indices]
test_labels <- iris$Species[-train_indices]

# Применение KNN (K=5)
knn_pred <- knn(train = train_data, 
                test = test_data, 
                cl = train_labels, 
                k = 5)

# Оценка результата
conf_matrix <- table(knn_pred, test_labels)
print("Матрица неточностей (K=5):")
print(conf_matrix)
```

### 2\. Сравнение зависимых групп, пример

#### Теория

  * **Назначение:** Проверка гипотез о различиях в средних значениях для **зависимых (связанных, парных)** выборок (например, измерения "до" и "после" на одних и тех же объектах).
  * **Метод (параметрический):** **Парный t-тест (Paired t-test)**. Используется, если разности нормально распределены.
  * **Метод (непараметрический):** **Критерий Уилкоксона для связанных выборок (Wilcoxon Signed-Rank Test)**. Используется, если предположения о нормальности нарушены.

#### Пример кода (R): Парный t-тест

Сравнение оценок студентов до и после курса.

```r
# Создание зависимых выборок (имитация данных)
set.seed(42)
score_before <- rnorm(30, mean = 70, sd = 10)
# После курса оценки немного выросли
score_after <- score_before + rnorm(30, mean = 5, sd = 5)

# Проведение парного t-теста
# H0: Среднее различие = 0
# Ha: Среднее различие != 0
t_test_result <- t.test(score_after, score_before, 
                        paired = TRUE, 
                        alternative = "two.sided")

print("Результат парного t-теста:")
print(t_test_result)

# Интерпретация: Если p-value < 0.05, отвергаем H0, 
# то есть есть статистически значимая разница.
```

-----

## 8\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Средства интеллектуального анализа данных

#### Теория

Средства (инструменты/техники) интеллектуального анализа данных (Data Mining) — это алгоритмы и методологии, используемые для поиска закономерностей.

  * **Классификация и Прогнозирование:** Логистическая регрессия, Деревья решений, Случайный лес, SVM, Нейронные сети.
  * **Кластеризация:** k-Means, Иерархическая кластеризация, DBSCAN.
  * **Ассоциативные правила:** Алгоритм Apriori.
  * **Снижение размерности:** Метод главных компонент (PCA), t-SNE.
  * **Обнаружение аномалий (Anomaly Detection):** Изоляционный лес, One-Class SVM.

#### Инструменты в R

R сам по себе является мощным средством. Конкретные пакеты:

  * **`caret`:** Универсальный интерфейс для многих моделей машинного обучения.
  * **`tidyverse`:** Набор пакетов (`ggplot2`, `dplyr`, `tidyr`) для манипуляции и визуализации данных.
  * **`rpart`/`randomForest`:** Для деревьев и ансамблевых методов.
  * **`e1071`:** Для SVM и наивного Байеса.

### 2\. Сравнение зависимых групп, пример

*(См. Билет 7, вопрос 2. Повторение материала.)*

#### Теория

  * **Назначение:** Проверка гипотез о различиях в средних значениях для **зависимых (связанных, парных)** выборок (измерения "до" и "после" на одних и тех же объектах).
  * **Параметрический:** **Парный t-тест** (если разности нормально распределены).
  * **Непараметрический:** **Критерий Уилкоксона для связанных выборок**.

#### Пример кода (R): Критерий Уилкоксона

Применяется, если нарушено предположение о нормальности разностей.

```r
# Создание зависимых выборок (имитация данных с возможным нарушением нормальности)
set.seed(10)
score_A <- runif(20, 50, 80)
score_B <- score_A + rpois(20, lambda = 3) # Случайное приращение

# Проведение критерия Уилкоксона для связанных выборок
# H0: Медиана разностей = 0
wilcox_test_result <- wilcox.test(score_B, score_A, 
                                  paired = TRUE, 
                                  alternative = "two.sided")

print("Результат критерия Уилкоксона:")
print(wilcox_test_result)

# Интерпретация: Если p-value < 0.05, отвергаем H0, 
# то есть есть статистически значимая разница в медианах.
```

-----

## 9\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Классификация задач анализа данных

Классификация задач анализа данных (Data Mining) по типу решаемой задачи:

1.  **Прогнозирование (Predictive Modeling):** Построение модели для предсказания значения целевой переменной на основе входных данных.
      * **Классификация (Classification):** Прогнозирование **категориальной** метки (например, клиент уйдет/не уйдет).
      * **Регрессия (Regression):** Прогнозирование **непрерывного** числового значения (например, цена акции).
2.  **Описание/Выявление закономерностей (Descriptive Modeling):** Нахождение и описание паттернов и взаимосвязей в данных.
      * **Кластеризация (Clustering):** Группировка схожих объектов (например, сегментация покупателей).
      * **Ассоциативные правила (Association):** Поиск взаимосвязей между элементами (например, "Если купил А и B, то купит C").
      * **Снижение размерности (Dimensionality Reduction):** Упрощение данных для визуализации или более эффективного моделирования (например, PCA).

### 2\. Нейронные сети и анализ данных, пример

#### Теория

  * **Назначение:** Универсальный инструмент для **классификации**, **регрессии** и **распознавания образов**.
  * **Принцип:** Состоят из слоев взаимосвязанных узлов (нейронов). Каждый нейрон получает входные сигналы, обрабатывает их с помощью весов и функции активации, и передает результат дальше. Обучение происходит через итерационную оптимизацию весов с использованием метода **обратного распространения ошибки (Backpropagation)**.
  * **Функция в R:** `nnet()` из пакета `nnet` (для простых сетей) или `keras`/`tensorflow` (для глубоких сетей).

#### Пример кода (R)

Использование многослойного персептрона для классификации ирисов.

```r
# Установка и загрузка пакета
library(nnet)
data(iris)

# Подготовка данных (преобразование в one-hot-encoding для целевой переменной)
# и нормализация (важно для НС)
iris_scaled <- as.data.frame(scale(iris[, 1:4]))
iris_target <- class.ind(iris$Species)
iris_data <- cbind(iris_scaled, iris_target)

# Разделение на тренировочный и тестовый наборы
set.seed(123)
train_indices <- sample(1:nrow(iris_data), 100)
train_data <- iris_data[train_indices, ]
test_data <- iris_data[-train_indices, ]

# Построение нейронной сети (1 скрытый слой, 5 узлов)
# Formula: Sepal.Length + ... -> setosa + versicolor + virginica
model_nn <- nnet(Species ~ ., 
                 data = train_data, 
                 size = 5,       # Количество нейронов в скрытом слое
                 rang = 0.1,     # Диапазон начальных весов
                 decay = 5e-4,   # Параметр регуляризации
                 maxit = 200,    # Максимальное количество итераций
                 trace = FALSE)

# Прогнозирование
predictions <- predict(model_nn, test_data, type = "class")

# Оценка результата
conf_matrix <- table(predictions, test_data$Species)
print("Матрица неточностей:")
print(conf_matrix)
```

-----

## 10\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Средства визуализации результатов анализа данных

#### Теория

Визуализация — критический этап в анализе данных, позволяющий понять структуру данных, выявить аномалии, оценить результаты моделирования и представить выводы.

  * **Этапы применения:**
      * **Разведочный анализ данных (Exploratory Data Analysis, EDA):** Для понимания распределений, корреляций и поиска пропусков/выбросов (гистограммы, диаграммы рассеяния, boxplots).
      * **Оценка модели:** Для оценки производительности модели (ROC-кривые, матрицы неточностей, остаточные графики).
      * **Представление результатов:** Для коммуникации выводов аудитории (линейные графики, столбчатые диаграммы, карты).

#### Основные средства/Пакеты в R

1.  **Базовые функции R (`plot()`, `hist()`):** Простые и быстрые, но ограниченные в настройке.
2.  **`ggplot2` (часть `tidyverse`):** Самый популярный и мощный пакет, основанный на **грамматике графики** (Grammar of Graphics). Позволяет создавать сложные, многослойные и высококачественные графики.
      * **Принцип:** График = Данные + Геометрия (`geom`) + Эстетика (`aes`) + Статистика + Масштаб + Координаты + Фасеты.
3.  **Интерактивная визуализация:** `plotly`, `shiny`.

#### Пример кода (R): `ggplot2`

```r
# Установка и загрузка пакета
library(ggplot2)
data(mtcars)

# Диаграмма рассеяния: зависимость mpg от hp, цвет по количеству цилиндров (cyl)
plot_mtcars <- ggplot(mtcars, 
                      aes(x = hp, y = mpg, color = factor(cyl))) +
  geom_point(size = 3) + # Точечная геометрия
  labs(title = "Зависимость расхода топлива от мощности",
       x = "Мощность (hp)",
       y = "Расход (mpg)",
       color = "Цилиндры") +
  theme_minimal() # Минималистичный стиль

print(plot_mtcars)
```

### 2\. Сравнение двух независимых групп, пример

#### Теория

  * **Назначение:** Проверка гипотез о различиях в средних значениях для **двух независимых** выборок (например, сравнение средней выручки у двух разных групп клиентов).
  * **Метод (параметрический):** **t-тест для независимых выборок (Two-sample t-test)**. Используется, если данные в каждой группе нормально распределены.
  * **Метод (непараметрический):** **Критерий Манна-Уитни (Mann-Whitney U Test, он же Wilcoxon Rank-Sum Test)**. Используется, если нарушены предположения о нормальности или равенстве дисперсий.

#### Пример кода (R): Независимый t-тест

Сравнение средней длины чашелистика (`Sepal.Length`) для двух видов ирисов.

```r
data(iris)

# Фильтрация данных для двух групп
virginica_data <- iris[iris$Species == "virginica", ]$Sepal.Length
versicolor_data <- iris[iris$Species == "versicolor", ]$Sepal.Length

# Проведение независимого t-теста
# H0: Средние длины чашелистика равны
# Ha: Средние длины чашелистика не равны
t_test_result <- t.test(virginica_data, versicolor_data, 
                        alternative = "two.sided", 
                        var.equal = FALSE) # var.equal=FALSE - тест Уэлча

print("Результат независимого t-теста:")
print(t_test_result)

# Интерпретация: Если p-value < 0.05, отвергаем H0.
```

-----

## 11\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Линейная регрессия и классификация, пример

*(Повторение материала из Билета 4, вопрос 2.)*

#### Теория

  * **Линейная регрессия (`lm`):** Только для **прогнозирования непрерывных** переменных.
  * **Классификация:** Для классификации **бинарной** переменной используется **Логистическая регрессия** (`glm` с `family = binomial`), так как она гарантирует, что прогнозируемые вероятности находятся в интервале $[0, 1]$.

#### Пример кода (R): Логистическая регрессия (для классификации)

```r
data(mtcars)

# Создание бинарной зависимой переменной (0 - автомат, 1 - ручная)
mtcars$am_binary <- mtcars$am

# Построение логистической регрессии: прогнозирование типа коробки передач (am) по мощности (hp)
model_logit <- glm(am_binary ~ hp, 
                   data = mtcars, 
                   family = binomial)

# Прогнозирование вероятностей (p > 0.5 -> ручная)
probabilities <- predict(model_logit, type = "response")
predictions <- ifelse(probabilities > 0.5, 1, 0)

# Оценка
accuracy <- mean(predictions == mtcars$am_binary)
print(paste("Точность классификации:", round(accuracy, 3)))
```

### 2\. Методы визуализации с использованием R

*(Повторение материала из Билета 10, вопрос 1.)*

#### Основные типы графиков и их назначение:

| Тип графика | Назначение | Пакет/Функция |
| :--- | :--- | :--- |
| **Гистограмма** | Распределение одной непрерывной переменной. | `hist()`, `geom_histogram()` |
| **Диаграмма рассеяния (Scatter Plot)** | Взаимосвязь между двумя непрерывными переменными. | `plot()`, `geom_point()` |
| **Ящичная диаграмма (Box Plot)** | Распределение переменной по категориям, выявление выбросов. | `boxplot()`, `geom_boxplot()` |
| **Столбчатая диаграмма (Bar Chart)** | Сравнение категориальных данных или подсчет частот. | `barplot()`, `geom_bar()` |
| **Линейный график** | Визуализация временных рядов или трендов. | `plot()`, `geom_line()` |

#### Пример кода (R): Box Plot с `ggplot2`

```r
library(ggplot2)
data(iris)

# Box Plot: сравнение распределения Sepal.Length по видам (Species)
ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_boxplot() +
  labs(title = "Box Plot длины чашелистика по видам") +
  theme_bw()
```

-----

## 12\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Множественная линейная регрессия, пример

*(Повторение материала из Билета 5, вопрос 2.)*

#### Теория

  * **Назначение:** Прогнозирование непрерывной зависимой переменной ($Y$) с использованием **двух или более** независимых переменных ($X_1, X_2, \dots$).
  * **Модель:** $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k + \varepsilon$.
  * **Функция в R:** `lm()`.

#### Пример кода (R)

Прогнозирование расхода топлива (`mpg`) по объему двигателя (`disp`), весу (`wt`) и числу цилиндров (`cyl`).

```r
data(mtcars)

# Построение модели множественной линейной регрессии
model_multi_lm <- lm(mpg ~ disp + wt + cyl, data = mtcars)

# Краткий вывод модели
summary(model_multi_lm)

# Извлечение скорректированного R-квадрат (Adjusted R-squared)
adj_r_squared <- summary(model_multi_lm)$adj.r.squared
print(paste("Скорректированный R-квадрат:", round(adj_r_squared, 3)))
```

### 2\. Рекомендательные системы, пример

#### Теория

  * **Назначение:** Прогнозирование, какой продукт или услуга будет интересен пользователю, на основе его предыдущих предпочтений или предпочтений других пользователей.
  * **Типы систем:**
    1.  **На основе содержимого (Content-Based):** Рекомендации схожих товаров на основе характеристик товара, которые нравились пользователю ранее.
    2.  **Коллаборативная фильтрация (Collaborative Filtering):**
          * **User-Based:** Рекомендации от "соседей", похожих пользователей.
          * **Item-Based:** Рекомендации товаров, похожих на те, что нравились пользователю.
    3.  **Гибридные системы:** Комбинация двух подходов.
  * **Пакет в R:** `recommenderlab`.

#### Пример кода (R): Коллаборативная фильтрация

Использование пакета `recommenderlab` для создания модели рекомендаций.

```r
# Установка и загрузка пакета
# install.packages("recommenderlab")
library(recommenderlab)
data(MovieLense) # Встроенный набор данных (рейтинги фильмов)

# Преобразование данных в формат ratingMatrix
r <- MovieLense[rowCounts(MovieLense) > 50, colCounts(MovieLense) > 100]
r <- r[1:50, 1:100] # Уменьшаем для скорости

# Построение модели User-Based Collaborative Filtering (UBCF)
model_ubcf <- Recommender(r, 
                          method = "UBCF") 

# Получение рекомендаций для первого пользователя (топ-5)
# Предполагаем, что 51-й пользователь - новый
user_id <- 51
recommendations <- predict(model_ubcf, r[user_id], n = 5)

print("Рекомендации (топ-5) для пользователя 51:")
print(as(recommendations, "list"))
```

-----

## 13\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Метод ближайших соседей (KNN), пример

*(Повторение материала из Билета 7, вопрос 1.)*

#### Теория

  * **Назначение:** **Классификация** (большинством голосов K соседей) и **Регрессия** (средним значением K соседей). Ленивый алгоритм.
  * **Ключевая особенность:** Чувствителен к **масштабу признаков** и **выбросам**. Требуется стандартизация/нормализация данных.

#### Пример кода (R)

```r
# Установка и загрузка пакета
library(class)
data(iris)

# Подготовка данных: стандартизация
iris_scaled <- scale(iris[, 1:4])

# Создание нового объекта для классификации
new_observation <- c(5.0, 3.5, 1.4, 0.2) # Напоминает Setosa

# Применение KNN (K=3)
knn_pred <- knn(train = iris_scaled, 
                test = new_observation, 
                cl = iris$Species, 
                k = 3)

print("Прогноз для нового наблюдения (K=3):")
print(knn_pred)
```

### 2\. Работа с массивами в R

#### Теория

  * **Массив (Array):** Обобщение векторов и матриц. Структура данных с **произвольным числом измерений** (N-мерный). Все элементы массива **должны быть одного типа**.
  * **Создание:** Функция `array(data, dim)`
      * `data`: Вектор элементов.
      * `dim`: Вектор, определяющий размерность (например, `c(3, 4, 2)` — 3 строки, 4 столбца, 2 "среза").
  * **Индексация:** Используется `[dim1, dim2, dim3, ...]`

#### Пример кода (R)

Создание и работа с 3D массивом (3 строки, 4 столбца, 2 среза).

```r
# Создание 3D массива (24 элемента)
A <- array(1:24, 
           dim = c(3, 4, 2))
print("Массив А (3x4x2):")
print(A)

# Выборка: Все элементы первого среза (первой матрицы)
slice_1 <- A[ , , 1]
print("Первый срез (3x4):")
print(slice_1)

# Выборка: Элемент (2-я строка, 3-й столбец, 2-й срез)
element <- A[2, 3, 2]
print("Элемент [2, 3, 2]:")
print(element)

# Применение функции к массиву (например, суммирование по срезам)
# MARGIN = 3 (суммирование по третьему измерению)
sum_across_slices <- apply(A, 
                           MARGIN = c(1, 2), 
                           FUN = sum)
print("Сумма по срезам (3x4 матрица):")
print(sum_across_slices)
```

-----

## 14\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Ассоциативные правила, пример

#### Теория

  * **Назначение:** Поиск скрытых взаимосвязей/паттернов в транзакционных данных (например, "анализ корзины покупок").
  * **Формат:** Если {A, B}, то {C} (If {Bread, Milk}, Then {Eggs}).
  * **Основные метрики:**
      * **Поддержка (Support):** Доля транзакций, содержащих {A и B}. $S(A \cup B) = \frac{\text{Кол-во}(A \cup B)}{\text{Кол-во всех транзакций}}$
      * **Достоверность (Confidence):** Доля транзакций с {A}, которые также содержат {B}. $C(A \to B) = \frac{S(A \cup B)}{S(A)}$
      * **Лифт (Lift):** Насколько часто {A} и {B} встречаются вместе, по сравнению с их случайной встречаемостью. **$Lift(A \to B) > 1$** указывает на положительную связь.
  * **Алгоритм:** **Apriori** (наиболее распространенный).
  * **Пакет в R:** `arules`.

#### Пример кода (R)

```r
# Установка и загрузка пакетов
# install.packages(c("arules", "arulesViz"))
library(arules)
data(Groceries) # Встроенные транзакционные данные

# Применение алгоритма Apriori
# min_support=0.01: минимум 1% транзакций содержат набор
# min_confidence=0.5: 50% достоверности
rules <- apriori(Groceries, 
                 parameter = list(supp = 0.01, conf = 0.5, minlen = 2))

# Сортировка правил по лифту (Lift)
rules_sorted <- sort(rules, by = "lift")

print("Топ-5 правил по Lift:")
inspect(rules_sorted[1:5]) 

# Пример интерпретации: {йогурт, сахар} => {молоко} (Lift=3.5) означает, 
# что покупатели, купившие йогурт и сахар, в 3.5 раза чаще покупают молоко, 
# чем в среднем по всем покупателям.
```

### 2\. Проверка статистических гипотез в среде R

#### Теория

Процесс использования статистических тестов для принятия решения об истинности или ложности некоторого утверждения (гипотезы) о совокупности.

  * **Нулевая гипотеза ($H_0$):** Утверждение, которое мы пытаемся опровергнуть (например, нет разницы между средними).
  * **Альтернативная гипотеза ($H_a$):** Утверждение, которое мы принимаем, если $H_0$ отклоняется (например, есть разница).
  * **P-значение (P-value):** Вероятность получить наблюдаемый или более экстремальный результат, при условии, что $H_0$ верна.
  * **Правило принятия решения:** Если **$P < \alpha$** (уровень значимости, обычно 0.05), то **отвергаем $H_0$**.

| Задача | Тест (Параметрический) | Функция в R |
| :--- | :--- | :--- |
| Сравнение 2 независимых средних | Независимый t-тест | `t.test(x, y)` |
| Сравнение 2 зависимых средних | Парный t-тест | `t.test(x, y, paired=TRUE)` |
| Сравнение \> 2 средних | Дисперсионный анализ (ANOVA) | `aov()` |
| Связь между 2 категориальными переменными | Критерий $\chi^2$ | `chisq.test()` |
| Проверка нормальности | Критерий Шапиро-Уилка | `shapiro.test()` |

#### Пример кода (R): Проверка на нормальность

```r
# Создание нормально распределенного вектора
data_norm <- rnorm(50, mean = 10, sd = 2)

# Критерий Шапиро-Уилка
# H0: Данные распределены нормально
shapiro_result <- shapiro.test(data_norm)
print("Результат критерия Шапиро-Уилка:")
print(shapiro_result)

# Если P-value > 0.05, H0 не отвергается -> данные можно считать нормально распределенными.
```

-----

## 15\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Алгоритм k-means, пример

#### Теория

  * **Назначение:** **Кластеризация** (неконтролируемое обучение). Группировка $N$ объектов в $K$ кластеров, где каждый объект принадлежит кластеру с ближайшим средним (центроидом).
  * **Принцип:** Итеративный процесс:
    1.  Случайно выбираются $K$ начальных центроидов.
    2.  **Шаг присвоения:** Каждый объект относится к ближайшему центроиду.
    3.  **Шаг обновления:** Центроид каждого кластера пересчитывается как среднее всех объектов, ему присвоенных.
    4.  Повторение шагов 2-3 до сходимости (центроиды перестают меняться).
  * **Недостатки:** Чувствителен к выбору начальных центроидов и требует предварительного задания $K$.
  * **Функция в R:** `kmeans()`.

#### Пример кода (R)

```r
data(iris)

# Кластеризация по первым 4 признакам (Species - это метка для сравнения)
data_for_kmeans <- iris[, 1:4]

# Выполнение k-means для K=3
set.seed(42) # Для воспроизводимости
kmeans_result <- kmeans(data_for_kmeans, 
                        centers = 3, 
                        nstart = 25) # Запуск 25 раз с разными начальными центроидами

# Просмотр результатов (размер кластеров)
print("Результаты k-means:")
print(kmeans_result)

# Визуализация кластеров (сравнение с реальными метками Species)
# install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans_result, 
             data = data_for_kmeans,
             geom = "point",
             main = "Кластеризация K-means (K=3)")
```

### 2\. Рекомендательные системы и анализ данных

*(Повторение материала из Билета 12, вопрос 2.)*

#### Теория

Рекомендательные системы — это прикладная область анализа данных, фокусирующаяся на прогнозировании предпочтений пользователя.

  * **Ключевые методы:**
      * **Коллаборативная фильтрация (Collaborative Filtering):** Основана на сходстве между пользователями или товарами. Требует много данных о взаимодействиях (ratings).
      * **Рекомендации на основе содержимого (Content-Based):** Основана на характеристиках самого товара и профиле пользователя.
  * **Метрики оценки:**
      * **Precision@k, Recall@k:** Точность и полнота в топ-K рекомендациях.
      * **RMSE:** Для оценки точности прогнозируемых рейтингов.

#### Пример кода (R): Добавление метрик

```r
# Установка и загрузка пакета
library(recommenderlab)
data(MovieLense) 

# Разделение данных для оценки
e <- evaluationScheme(MovieLense, 
                      method = "split", 
                      train = 0.9, 
                      k = 1, # k=1 означает один тестовый набор для каждого пользователя
                      given = 10) # 10 известных рейтингов для прогноза

# Обучение модели
r_ubcf <- Recommender(getData(e, "train"), "UBCF")

# Прогнозирование и оценка
results <- evaluate(e, method = "UBCF", type = "topNList", n = c(1, 3, 5))

# Визуализация результатов оценки (например, ROC-кривая)
# plot(results, annotate = 1, legend = "topleft")

print("Средний Precision и Recall для N=5:")
getConfusionMatrix(results)[[1]]
```

-----

## 16\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Алгоритм kNN, пример

*(Повторение материала из Билета 7, вопрос 1.)*

#### Теория

  * **Назначение:** **Классификация** (по большинству голосов $K$ ближайших соседей).
  * **Метрика расстояния:** Обычно Евклидово расстояние.
  * **Подготовка данных:** **Масштабирование/нормализация** обязательна для предотвращения доминирования признаков с большим диапазоном.

#### Пример кода (R)

```r
library(class)
data(iris)

# Масштабирование признаков
iris_scaled <- scale(iris[, 1:4])

# Создание тестового набора
test_index <- 1:5
test_data <- iris_scaled[test_index, ]
test_labels <- iris$Species[test_index]
train_data <- iris_scaled[-test_index, ]
train_labels <- iris$Species[-test_index]

# Применение KNN (K=10)
knn_pred <- knn(train = train_data, 
                test = test_data, 
                cl = train_labels, 
                k = 10)

print("Прогноз KNN (K=10):")
print(knn_pred)
print("Реальные метки:")
print(test_labels)
```

### 2\. Дисперсионный анализ, пример

#### Теория

  * **Назначение:** **Сравнение средних значений более чем двух независимых групп** (однофакторный ANOVA) или анализ влияния нескольких факторов и их взаимодействия (многофакторный ANOVA).
  * **Нулевая гипотеза ($H_0$):** Средние значения всех групп равны.
  * **Принцип:** Разделение общей дисперсии переменной на дисперсию "между группами" (объясненная фактором) и дисперсию "внутри групп" (остаточная/ошибочная). Сравнение этих дисперсий с помощью F-статистики.
  * **Функция в R:** `aov()` (Analysis of Variance).
  * **Предположения:** Нормальность остатков, гомоскедастичность (равенство дисперсий).

#### Пример кода (R): Однофакторный ANOVA

Сравнение средней длины чашелистика (`Sepal.Length`) для всех трех видов ирисов (`Species`).

```r
data(iris)

# Выполнение однофакторного ANOVA
# Formula: зависимая переменная ~ фактор
anova_model <- aov(Sepal.Length ~ Species, data = iris)

# Вывод результатов
summary(anova_model)

# Интерпретация: Если P-value для фактора Species < 0.05, 
# то отвергаем H0. Есть статистически значимая разница в 
# средних длинах чашелистика между видами.

# Пост-хок тест (если ANOVA значим) для выявления, какие именно пары различаются
TukeyHSD(anova_model)
```

-----

## 17\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Кластерный анализ, пример

#### Теория

  * **Назначение:** **Кластеризация** (неконтролируемое обучение). Группировка объектов таким образом, что объекты в одном кластере схожи между собой и не схожи с объектами в других кластерах.
  * **Основные типы:**
    1.  **Разделяющая кластеризация (Partitioning):** k-Means (фиксированное число $K$ кластеров).
    2.  **Иерархическая кластеризация (Hierarchical):** Создание древовидной структуры (дендрограммы), показывающей последовательное слияние/разделение кластеров.
          * **Агломеративная (Agglomerative):** Начинает с отдельных объектов, последовательно объединяя их.
          * **Дивизивная (Divisive):** Начинает с одного кластера, последовательно разделяя его.
  * **Пакеты в R:** `stats` (`kmeans`, `hclust`), `factoextra`.

#### Пример кода (R): Иерархическая кластеризация

```r
data(iris)
data_scaled <- scale(iris[, 1:4])

# 1. Расчет матрицы расстояний
dist_matrix <- dist(data_scaled, method = "euclidean")

# 2. Агломеративная кластеризация (метод Уорда)
hc_model <- hclust(dist_matrix, method = "ward.D2")

# 3. Визуализация дендрограммы
plot(hc_model, 
     hang = -1, 
     main = "Иерархическая кластеризация (метод Ward.D2)",
     labels = FALSE)

# 4. Выделение 3 кластеров
rect.hclust(hc_model, k = 3, border = 2:4)

# 5. Присвоение кластеров
cluster_groups <- cutree(hc_model, k = 3)
```

### 2\. Проверка статистических гипотез в среде R

*(Повторение материала из Билета 14, вопрос 2.)*

#### Теория

  * **Основной принцип:** Оценка $P$-значения для принятия решения об отклонении Нулевой гипотезы ($H_0$).
  * **Непараметрические тесты:** Используются, когда нарушены предположения о распределении (например, нормальность).
      * **Критерий Манна-Уитни** (`wilcox.test()`): Сравнение 2 независимых групп.
      * **Критерий Крускала-Уоллиса** (`kruskal.test()`): Сравнение \> 2 независимых групп.
      * **Критерий Уилкоксона** (`wilcox.test(paired=TRUE)`): Сравнение 2 зависимых групп.

#### Пример кода (R): Критерий $\chi^2$ (Хи-квадрат)

Проверка связи между двумя категориальными переменными (пол и курение).

```r
# Создание таблицы сопряженности (имитация данных)
smoking_data <- matrix(c(50, 20, 30, 40), 
                       nrow = 2, 
                       dimnames = list(Sex = c("Male", "Female"),
                                       Smoking = c("Smoker", "Non-Smoker")))
print("Таблица сопряженности:")
print(smoking_data)

# Критерий Хи-квадрат
# H0: Переменные независимы (нет связи)
chisq_result <- chisq.test(smoking_data)
print("Результат критерия Хи-квадрат:")
print(chisq_result)

# Если P-value < 0.05, отвергаем H0. Переменные (пол и курение) связаны.
```

-----

## 18\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Требования к данным, подготовка данных, пример

#### Теория

Подготовка данных (Data Preprocessing) занимает до 80% времени аналитика. Требования к данным:

1.  **Полнота (Completeness):** Минимум пропущенных значений.
2.  **Чистота/Корректность (Accuracy/Cleanliness):** Отсутствие ошибок, опечаток, дубликатов, выбросов.
3.  **Единообразие (Consistency):** Единый формат и единицы измерения (например, даты, валюты).
4.  **Релевантность (Relevance):** Данные должны быть применимы для решения поставленной задачи.

#### Основные этапы подготовки:

1.  **Очистка данных:** Работа с пропусками, выбросами, дубликатами.
2.  **Трансформация/Преобразование:** Нормализация/стандартизация (для дистанционных методов), логарифмирование (для выравнивания распределения), агрегация.
3.  **Снижение размерности:** PCA или отбор признаков.
4.  **Кодирование категориальных признаков:** Преобразование текстовых категорий в числа (например, One-Hot Encoding, Label Encoding).

#### Пример кода (R): Кодирование и стандартизация

```r
data <- data.frame(
  Feature1 = c(10, 20, 30, 40, 50),
  Category = c("A", "B", "A", "C", "B")
)

# 1. Стандартизация (Z-score) числового признака
data$Feature1_scaled <- scale(data$Feature1)

# 2. One-Hot Encoding категориального признака (для моделей)
# install.packages("caret")
library(caret)
dummy_vars <- dummyVars(~ Category, data = data)
data_onehot <- predict(dummy_vars, newdata = data)
data_transformed <- cbind(data[, "Feature1_scaled", drop = FALSE], 
                          as.data.frame(data_onehot))

print("Преобразованные данные:")
print(data_transformed)
```

### 2\. Нейронные сети и анализ данных, пример

*(Повторение материала из Билета 9, вопрос 2.)*

#### Теория

  * **Типы сетей:**
      * **Многослойный персептрон (MLP):** Для классификации/регрессии.
      * **Сверточные нейронные сети (CNN):** Для анализа изображений.
      * **Рекуррентные нейронные сети (RNN):** Для анализа последовательностей (текст, временные ряды).
  * **Функция активации:** Определяет выход нейрона (Sigmoid, ReLU, Tanh).
  * **Обучение:** С помощью оптимизаторов (Adam, SGD) и функции потерь (Cross-Entropy, MSE).

#### Пример кода (R): Использование `neuralnet`

```r
# install.packages("neuralnet")
library(neuralnet)
data(iris)

# Нормализация
maxs <- apply(iris[, 1:4], 2, max)
mins <- apply(iris[, 1:4], 2, min)
iris_norm <- as.data.frame(scale(iris[, 1:4], center = mins, scale = maxs - mins))

# One-Hot Encoding для целевой переменной
iris_target <- model.matrix(~ Species - 1, data = iris)
iris_final <- cbind(iris_norm, iris_target)

# Построение сети (4 входа, 2 скрытых нейрона, 3 выхода)
nn_model <- neuralnet(
  Speciessetosa + Speciesversicolor + Speciesvirginica ~ 
    Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
  data = iris_final,
  hidden = 2, # Один скрытый слой с 2 нейронами
  linear.output = FALSE, # Логистическая функция для классификации
  stepmax = 1e+07,
  threshold = 0.01
)

# Визуализация сети
plot(nn_model)
```

-----

## 19\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Основные понятия интеллектуального анализа данных

#### Теория

Интеллектуальный анализ данных (Data Mining) — это процесс обнаружения закономерностей в больших наборах данных, включающий методологию и прикладные области.

  * **Знание (Knowledge):** Закономерности, извлеченные из данных.
  * **Шаблон (Pattern):** Регулярность или структура, обнаруженная в данных (например, кластер, правило ассоциации).
  * **Целевая переменная (Target Variable):** Переменная, которую мы хотим предсказать (в контролируемом обучении).
  * **Признак/Предиктор (Feature/Predictor):** Входная переменная, используемая для прогнозирования или описания.
  * **Контролируемое обучение (Supervised Learning):** Обучение на данных с размеченными целевыми переменными (Классификация, Регрессия).
  * **Неконтролируемое обучение (Unsupervised Learning):** Обучение на неразмеченных данных, поиск скрытой структуры (Кластеризация, Снижение размерности).
  * **Полуконтролируемое обучение (Semi-Supervised Learning):** Использование небольшого количества размеченных и большого количества неразмеченных данных.

### 2\. Нейронные сети и анализ данных, пример

*(Повторение материала из Билета 18, вопрос 2.)*

#### Пример кода (R): Прогнозирование

Продолжение примера с `neuralnet` (Билет 18, вопрос 2).

```r
# ... (код построения nn_model из Билета 18)

# Прогнозирование на тренировочном наборе (для простоты)
nn_predict <- compute(nn_model, 
                      iris_final[, 1:4])

# Извлечение вероятностей
predicted_prob <- nn_predict$net.result

# Преобразование вероятностей в классы
predicted_class <- colnames(iris_target)[apply(predicted_prob, 1, which.max)]

# Реальные классы
actual_class <- iris$Species

# Оценка
conf_matrix <- table(predicted_class, actual_class)
print("Матрица неточностей:")
print(conf_matrix)
```

-----

## 20\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Алгоритм PCA, пример

*(Повторение материала из Билета 2, вопрос 2.)*

#### Теория

  * **Назначение:** **Снижение размерности** (построение нового базиса). Сохраняет максимальное количество дисперсии (информации) исходных данных в меньшем числе компонент.
  * **Масштабирование:** Обязательно, так как PCA чувствителен к масштабу признаков.
  * **Визуализация:** Используется для построения 2D или 3D графиков данных.

#### Пример кода (R)

```r
data(iris)

# Применение PCA к числовым данным
iris.pca <- prcomp(iris[, 1:4], 
                   center = TRUE, 
                   scale. = TRUE)

# Просмотр вклада каждой компоненты (Scree Plot)
plot(iris.pca, 
     type = "l", 
     main = "График вклада компонент (Scree Plot)")

# Получение координат объектов в новом пространстве (Principal Component Scores)
head(iris.pca$x)

# Использование пакета ggplot2 для более красивой визуализации PCA
library(ggplot2)
data_pca <- as.data.frame(iris.pca$x)
data_pca$Species <- iris$Species

ggplot(data_pca, aes(x = PC1, y = PC2, color = Species)) +
  geom_point() +
  labs(title = "PCA: Визуализация ирисов на PC1 и PC2") +
  theme_minimal()
```

### 2\. Очистка данных, пример

#### Теория

Очистка данных — это процесс исправления или удаления неверных, неполных, неточных или нерелевантных частей данных.

  * **Этапы:**
    1.  **Обработка пропусков (Missing Values):** Удаление строк/столбцов или заполнение (импутация).
    2.  **Обработка выбросов (Outliers):** Выявление и сглаживание/удаление (например, с использованием IQR или Z-score).
    3.  **Обработка дубликатов (Duplicates):** Идентификация и удаление.
    4.  **Сглаживание (Smoothing):** Удаление шума из данных (например, агрегация или биннинг).

#### Пример кода (R): Удаление дубликатов и выбросов

```r
# Создание данных с дубликатом и выбросом
data <- data.frame(
  ID = 1:5,
  Score = c(10, 20, 30, 20, 150) # 150 - выброс, 20 - дубликат
)

# 1. Удаление дубликатов
data_unique <- unique(data)
print("После удаления дубликатов:")
print(data_unique)

# 2. Обработка выбросов (на примере Score)
# Расчет квартилей и IQR
Q1 <- quantile(data_unique$Score, 0.25)
Q3 <- quantile(data_unique$Score, 0.75)
IQR_val <- Q3 - Q1

# Определение границ для выбросов
lower_bound <- Q1 - 1.5 * IQR_val
upper_bound <- Q3 + 1.5 * IQR_val

# Фильтрация данных (удаление выбросов)
data_clean <- data_unique[data_unique$Score >= lower_bound & 
                            data_unique$Score <= upper_bound, ]

print("После удаления выбросов (с использованием IQR):")
print(data_clean)
```

-----

## 21\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Алгоритм k-means, пример

*(Повторение материала из Билета 15, вопрос 1.)*

#### Теория

  * **Метод выбора K (локоть/Elbow Method):** Построение графика зависимости суммы квадратов расстояний внутри кластеров (Within-Cluster Sum of Squares, WCSS) от числа кластеров $K$. Точка изгиба ("локоть") указывает на оптимальное $K$.

#### Пример кода (R): Использование локтевого метода

```r
data(iris)
data_for_kmeans <- iris[, 1:4]

# Функция для вычисления WCSS для заданного K
wss <- function(k) {
  kmeans(data_for_kmeans, k, nstart = 10)$tot.withinss
}

# Расчет WCSS для K от 1 до 10
k_values <- 1:10
wcss_values <- sapply(k_values, wss)

# Построение графика "Локоть"
plot(k_values, 
     wcss_values, 
     type = "b", 
     xlab = "Количество кластеров (K)",
     ylab = "Сумма квадратов внутрикластерных расстояний (WCSS)",
     main = "Метод локтя для K-means")
# На графике видно, что "локоть" находится в K=3 (для Iris)
```

### 2\. Процесс анализа данных

#### Теория

Анализ данных — это систематический процесс сбора, очистки, преобразования, моделирования и интерпретации данных с целью получения полезной информации, которая поддерживает принятие решений. Типичный циклический процесс (часто называемый **CRISP-DM** — Cross-Industry Standard Process for Data Mining) включает:

1.  **Понимание задачи (Business Understanding):** Определение целей, требований и критериев успеха проекта.
2.  **Понимание данных (Data Understanding):** Сбор данных, разведочный анализ (EDA), проверка качества данных.
3.  **Подготовка данных (Data Preparation):** Очистка, трансформация, выборка, кодирование.
4.  **Моделирование (Modeling):** Выбор и обучение алгоритмов (регрессия, классификация, кластеризация).
5.  **Оценка (Evaluation):** Оценка качества модели, интерпретация результатов с точки зрения исходной задачи.
6.  **Внедрение (Deployment):** Внедрение модели в рабочую среду и мониторинг ее производительности.

**Ключевые пакеты в R, соответствующие этапам:**

  * **Понимание данных/EDA:** `dplyr`, `ggplot2`.
  * **Подготовка данных:** `tidyr`, `stringr`, `caret` (для препроцессинга).
  * **Моделирование:** `caret`, `glm`, `randomForest`, `nnet`, `recommenderlab`.
  * **Оценка/Отчетность:** `ggplot2`, `knitr`, `rmarkdown`.

-----

## 22\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Алгоритм kNN, пример

*(Повторение материала из Билета 16, вопрос 1.)*

#### Теория

  * **KNN для регрессии:** Прогнозируемое значение — это **среднее арифметическое** или **взвешенное среднее** (где веса обратно пропорциональны расстоянию) значений целевой переменной у $K$ ближайших соседей.

#### Пример кода (R): KNN для регрессии

```r
library(FNN) # Пакет для KNN (для регрессии часто используется этот)
data(mtcars)

# Нормализация данных
mtcars_norm <- as.data.frame(scale(mtcars[, -1]))
target_norm <- mtcars$mpg

# Разделение
set.seed(123)
train_indices <- sample(1:nrow(mtcars), 20)
train_data <- mtcars_norm[train_indices, ]
test_data <- mtcars_norm[-train_indices, ]
train_target <- target_norm[train_indices]
test_target <- target_norm[-train_indices]

# Применение KNN для регрессии (K=3)
knn_reg_pred <- knn.reg(train = train_data, 
                        test = test_data, 
                        y = train_target, 
                        k = 3)$pred

# Оценка (Root Mean Squared Error, RMSE)
rmse <- sqrt(mean((knn_reg_pred - test_target)^2))
print(paste("RMSE KNN регрессии (K=3):", round(rmse, 3)))
```

### 2\. Подготовка данных. Работа с пропусками в R

*(Повторение материала из Билета 3, вопрос 1.)*

#### Теория

Пропущенные значения (`NA`) — распространенная проблема.

  * **Идентификация:** `is.na()`, `sum(is.na(data))`, `sapply(data, function(x) sum(is.na(x)))`.
  * **Обработка:**
      * **`na.omit(data)`:** Удаляет строки с пропусками.
      * **Импутация (Imputation):** Замена пропусками.
          * **Пакет `zoo`:** `na.aggregate(data, FUN = mean)` (заполнение средним).
          * **Пакет `mice`:** Для продвинутой множественной импутации на основе моделей.

#### Пример кода (R): Множественная импутация с `mice`

```r
# Установка и загрузка пакета
# install.packages("mice")
library(mice)
data(nhanes) # Встроенный набор данных с пропусками

# Выполнение множественной импутации (5 итераций, PMM - Predictive Mean Matching)
imputed_data <- mice(nhanes, 
                     m = 5,       # Количество наборов данных
                     maxit = 50,  # Количество итераций
                     meth = 'pmm', # Метод
                     seed = 500)

# Просмотр пропусков (теперь заполнены)
print("Статус пропусков после импутации:")
print(imputed_data$loggedEvents)

# Получение первого заполненного набора данных
data_completed <- complete(imputed_data, 1) 
print("Первые строки заполненного набора данных:")
head(data_completed)
```

-----

## 23\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Задачи аналитики данных, пример

*(Повторение материала из Билета 1, вопрос 1.)*

#### Теория

Задачи аналитики данных включают:

| Задача | Тип обучения | Ключевая цель | Пример |
| :--- | :--- | :--- | :--- |
| **Классификация** | Контролируемое | Прогноз категориальной метки | Предсказание, купит ли клиент продукт (Да/Нет). |
| **Регрессия** | Контролируемое | Прогноз непрерывного значения | Прогноз продаж на следующий месяц. |
| **Кластеризация** | Неконтролируемое | Сегментация, поиск групп | Группировка клиентов по поведению. |
| **Снижение размерности** | Неконтролируемое | Сжатие данных, визуализация | Уменьшение 100 признаков до 2 для графика. |
| **Обнаружение аномалий** | Разное | Поиск редких, необычных объектов | Выявление мошеннических транзакций. |

#### Пример задачи и модели (R)

Задача: **Классификация** (бинарная).
Цель: Прогноз, переживет ли пассажир "Титаника" (`Survived`).
Модель: Логистическая регрессия.

```r
# library(titanic)
# data(titanic_train) 
# ... (Код очистки и предобработки)

# Пример: Прогноз выживания по полу (Sex) и возрасту (Age)
# model_survival <- glm(Survived ~ Sex + Age, data = titanic_train, family = binomial)
```

### 2\. Алгоритм кластеризации, пример

*(Повторение материала из Билета 17, вопрос 1.)*

#### Пример кода (R): Использование DBSCAN (Density-Based Spatial Clustering)

DBSCAN — не требует задания $K$, находит кластеры произвольной формы и хорошо справляется с шумом (выбросами).

```r
# Установка и загрузка пакета
# install.packages("dbscan")
library(dbscan)
data(iris)
data_scaled <- scale(iris[, 1:4])

# Применение DBSCAN
# eps: максимальное расстояние между двумя объектами, чтобы один считался соседом другого
# MinPts: минимальное количество соседей для формирования кластера
dbscan_result <- dbscan(data_scaled, 
                        eps = 0.4, 
                        minPts = 5)

# Просмотр результатов
print("Результаты DBSCAN:")
print(dbscan_result)

# Визуализация (с использованием PCA для 2D)
library(factoextra)
fviz_cluster(dbscan_result, 
             data = data_scaled,
             stand = FALSE, 
             geom = "point",
             main = "Кластеризация DBSCAN")
# Кластер 0 - это шумовые точки
```

-----

## 24\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Построение графиков в среде R

*(Повторение материала из Билета 10, вопрос 1.)*

#### Теория

  * **Базовый R:** Быстрый просмотр данных (`plot`, `hist`).
  * **`ggplot2`:** Профессиональная визуализация на основе слоев (данные, эстетика, геометрия).

#### Пример кода (R): Многослойный график с `ggplot2`

Построение графика рассеяния с линией тренда и разделением по категориям.

```r
library(ggplot2)
data(mtcars)

# График рассеяния (mpg vs. wt), цвет по цилиндрам, сглаженная линия тренда
plot_complex <- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_point(size = 3) + 
  geom_smooth(method = "lm", se = FALSE) + # Добавление линейного тренда
  facet_wrap(~ cyl) + # Разделение на отдельные графики по цилиндрам
  labs(title = "MPG vs. Weight по количеству цилиндров",
       x = "Вес (тыс. фунтов)",
       y = "Расход (миль/галлон)",
       color = "Цилиндры") +
  theme_classic()

print(plot_complex)
```

### 2\. Линейная регрессия, пример

*(Повторение материала из Билета 4, вопрос 2.)*

#### Теория

  * **Простая Линейная Регрессия:** Связь между одной независимой и одной зависимой переменной.
  * **Модель:** $\hat{Y} = \beta_0 + \beta_1 X$.
  * **Оценка:** Метод наименьших квадратов (Ordinary Least Squares, OLS).

#### Пример кода (R)

```r
data(mtcars)

# Построение модели: прогнозирование расхода (mpg) по весу (wt)
model_simple_lm <- lm(mpg ~ wt, data = mtcars)

# Вывод коэффициентов:
# Intercept (beta0) - ожидаемое mpg при wt=0
# wt (beta1) - изменение mpg при увеличении wt на 1
print(coef(model_simple_lm))

# R-квадрат: доля дисперсии Y, объясненная моделью X
r_squared <- summary(model_simple_lm)$r.squared
print(paste("R-квадрат:", round(r_squared, 3)))
```

-----

## 25\. ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ

### 1\. Средства визуализации для аналитики данных

*(Повторение материала из Билета 10, вопрос 1.)*

#### Теория

Визуализация позволяет:

1.  **Обнаружить:** Неожиданные закономерности, выбросы и ошибки в данных (EDA).
2.  **Проверить:** Соответствие данных статистическим предположениям (например, нормальность).
3.  **Объяснить:** Результаты моделирования и сделать выводы доступными.

#### Ключевые принципы хорошей визуализации

  * **Ясность (Clarity):** График должен быть легко читаемым.
  * **Точность (Accuracy):** Визуальное представление должно точно отражать данные (не искажать пропорции).
  * **Контекст (Context):** Наличие заголовков, осей, легенд.
  * **Минимум "чернил" (Data-Ink Ratio):** Максимизация чернил, представляющих данные, минимизация декоративных элементов.

#### Пример кода (R): Географическая визуализация (пример)

```r
# install.packages(c("maps", "ggplot2"))
library(maps)
library(ggplot2)

# Загрузка данных карты США
us_states <- map_data("state")

# Создание простого картографического графика
ggplot(us_states, aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = "white", color = "black") + 
  coord_fixed(1.3) + # Фиксированное соотношение сторон
  labs(title = "Карта штатов США") +
  theme_void()
# В реальной аналитике: раскрашивание регионов по показателю (например, плотности населения).
```

### 2\. Нейронные сети и анализ данных, пример

*(Повторение материала из Билета 18, вопрос 2.)*

#### Теория

Нейронные сети являются мощным, но "черным ящиком" (Black-Box) методом, что затрудняет интерпретацию, в отличие от линейной регрессии или деревьев решений.

  * **Глубокое обучение (Deep Learning):** Нейронные сети с несколькими скрытыми слоями. Требуют больших объемов данных и вычислительных ресурсов (часто используются с `keras` / `tensorflow` в R).

#### Пример кода (R): Прогнозирование с `neuralnet` и визуализация ошибки

```r
# ... (используем модель nn_model из Билета 18, вопрос 2)

# Визуализация ошибки обучения (полезно для оценки сходимости)
# nn_model$restarts - содержит информацию об ошибках

# Построение графика зависимости ошибки от итерации
plot(nn_model$net.result[[1]]$.residuals, 
     type = "l", 
     main = "График ошибок обучения",
     xlab = "Итерация",
     ylab = "Сумма квадратов ошибок (SSE)")
```