## 1. Интерполяционная формула Лагранжа. Погрешность.

### Теория

Пусть задана $n+1$ пара чисел $(x_0,y_0),(x_1,y_1),\dots,(x_n,y_n)$,
Интерполяционный многочлен Лагранжа рассматривается в форме: $$
L_n(x)=\Sigma^{n}_{i=0}y_il_i(x)$$
где базисные полиномы $l_i$ определяются по формуле:

$$
l_i(x) = \prod_{j=0_, l \ne i}^{n}\frac{x-x_j}{x_i-x_j}
$$

#### Случай равностоящих услов интерполяции

$$
l_j(x)= \prod_{j=0_, l \ne i}^{n}\frac{x-x_j}{x_i-x_j} = \frac{\prod_{j=0_, l \ne i}^{n}(x-x_0-ih)}{h^n\prod^{n}_{i=0,i\ne j}(j-i)}
$$

**Остаток многочлена Лагранжа** - ошибка (погрешность), которая допускается, когда функция заменяется многочленом.
$$R_n(x)=f(x) - L_n(x) = \frac{f^{n+1}(\xi)}{(n+1)!}(x - x_0)\dots(x-x_n)$$
Абсолютная погрешность

$$
|R_n(x)|\le\frac{M^{(n+1)}(x)}{(n+1)!}*|\omega_n(x)|
$$

где

$$
M^{(n+1)}(x)=max_{a\le x \le b}|f^{n+1}(x)|
$$

$$
|\omega_n(x)|=\prod_{j=0}^n(x-x_j)
$$

## 2. Интерполяционные формулы Ньютона. Погрешность.

Многочлен Ньютона представляет из себя форму многочлена Лагранжа.

$$
P_n(x) = f(x_0)+(x-x_0)f(x_0, x_1) + (x-x_0)(x-x_1)f(x_0,x_1,x_2)
+...(x-x_0)(x-x_1)...(x-x_{n-1})f(x_0,x_1,...,x_n)
$$

Вторая интерполяционная формула Ньютона:
$$P_n(x)=y_n+t\Delta y_{n-1}+\frac{t(t+1)}{2!}\Delta^2y_{n-2}+...+\frac{t(t+1)(t+2)...(t+n-1)}{n!}\Delta^ny_0$$
Здесь $$\Delta^ky_m=\Delta^{k-1}y_{m+1}-\Delta^{k-1}y_m : y_i=f(x_i)$$
$t = \frac{x-x_0}{h}$, $\Delta^ny_i$- конечные разности.

Остаточный член:
$$R_n(x)=f(x) - P_n(x) = \frac{f^{n+1}(\xi)}{(n+1)!}(x - x_0)\dots(x-x_n)$$
Абсолютная погрешность

$$
|R_n(x)|\le\frac{M^{(n+1)}(x)}{(n+1)!}*|\omega_n(x)|
$$

где

$$
M^{(n+1)}(x)=max_{a\le x \le b}|f^{n+1}(x)|
$$

$$
|\omega_n(x)|=\prod_{j=0}^n(x-x_j)
$$

## 3. Оптимальный выбор узлов интерполяции. Многочлен Чебышева.

С помощью корректногошен выбора узлов можно минимизировать $\omega_n(x)$ в оценке погрешности, тем самым повысить точность интерполяции. Данная задача может быть решена с помощью многочлена Чебышева:

$$
T_{n+1}=\frac{b-a}{2^{2n+1}}*cos \bigg((n+1)*arccos\Big(\frac{2x-(b+a)}{b-a}\Bigr) \biggr)
$$

В качестве услов следует взять корни этого многочлена, то есть точки:

$$
x_i=\frac{a+b}{2}+\frac{b-a}{2}*cos\Big(\frac{2i+1}{2(n+1)}*\pi\Big)
$$

Явные формы:

$$
T_n(x)=\frac{(x+\sqrt{x^2-1})^n+(x-\sqrt{x^2-1})^n}{2}=\Sigma_{k=0}^{[n/2]}\frac{n!}{2k!(n-2k)!}(x^2-1)^kx^{n-2k}
$$

$$
U_n(x)=\frac{(x+\sqrt{x^2-1})^{n+1}+(x-\sqrt{x^2-1})^{n+1}}{2}=\Sigma_{k=0}^{[n/2]}\frac{(n+1)!}{(2k+1)!(n-2k)!}(x^2-1)^kx^{n-2k}
$$

## 4. Сплайн-интерполяция. Обратное интерполирование.

### Сплайн-интерполяция

Интерполяция высокими многочленами часто приводит к эффекту Рунге — резким колебаниям аппроксимирующей функции на концах интервала. Для устранения этого недостатка применяется **сплайн-интерполяция**, при которой функция аппроксимируется не одним глобальным многочленом, а **набором многочленов малой степени**, определённых на отдельных подынтервалах.

Сплайн — это кусочно-полиномиальная функция, обладающая заданной степенью гладкости.

Наиболее распространён **кубический сплайн**, так как он обеспечивает компромисс между гладкостью и вычислительной сложностью.

На каждом интервале $[x_i,x_{i+1}]$:
$S_i(x)=a_i+b_i(x-x_i)+c_i(x-x_i)^2+d_i(x-x_i)^3$

### Условия построения кубического сплайна

Для определения коэффициентов используются следующие условия:

1. **Интерполяция значений функции**
   $S(x*i)=y_i, \quad S(x_{i+1})=y_{i+1}$
2. **Гладкость первой производной**
   $S'_i(x_{i+1})=S'_{i+1}(x_{i+1})$

3. **Гладкость второй производной**
   $S''_i(x_{i+1})=S''_{i+1}(x_{i+1})$

4. **Краевые условия**
-   естественный сплайн: $S''(x_0)=S''(x_n)=0$
-   закреплённый сплайн: заданы $S'(x_0)$ и $S'(x_n)$

В результате получается система линейных уравнений с трёхдиагональной матрицей, которая эффективно решается методом прогонки.

### Свойства сплайнов
-   высокая точность аппроксимации
-   отсутствие паразитных осцилляций
-   локальность изменения (изменение одного узла не влияет на весь интервал)

### Обратное интерполирование

Обратное интерполирование — задача определения значения аргумента $x$ по заданному значению функции $y$.

Формально требуется решить уравнение:\
$f(x)=y^*$

Методы решения:
1. Перестановка ролей $x$ и $y$ (при монотонности функции)
2. Использование интерполяционного многочлена
3. Численные методы решения нелинейных уравнений

## 5. Квадратурные формулы Ньютона-Котеса.

Набор формул для численного интегрирования, основанных на вычислении интегрируемой функции в одинакого отстоящих друг от другой $n+1$ точках.
Типы:

1. $x_0=a$ и $x_n=b$ - замкнутный тип
2. $x_0>a$ и $x_n<b$ - открытый тип
    $$
    \int_{a}^{b}f(x)dx\approx\Sigma_{i=0}^{n}A_if(x_i)
    $$
    Где

-   замкнутые $x_i=a+ih$, где $h=\frac{b-a}{n}$
-   открытые $x_i = a + (i+1)h$, где $h=\frac{b-a}{n+2}$
    $h$ - размер шага, $A_i$ - квадратурный коэффициент, который можно вычислить как интегралы от базисных многочленов Лагранжа.
    $$
    \int_a^bf(x)dx \approx \int_a^bL_n(x)=\int_a^b\Sigma^{n}_{i=0}f_il_i(x)=\Sigma_{i=0}^n\int_a^bl_i(x)dx
    $$
    $$
    \int_a^bl_i(x)dx = A_i
    $$
    $$
    l_i(x) = \prod_{j=0_, l \ne i}^{n}\frac{x-x_j}{x_i-x_j}
    $$
    Формулы не устройчивы на высоких степенях, так как на них ошибка растёт экспоненциально. Также устойчивые формулы Ньютона — Котса можно построить, если заменить интерполяцию на метод наименьших квадратов. Это позволяет записать численно устойчивые формулы даже для высоких степеней.

## 6. Формулы левых, правых, средних прямоугольников. Геометрический смысл. Погрешности.

Для этого метода необходимо разбить промежуток интегрирования на некоторое количество отрезков (чем больше - тем лучше). На каждом отрезке строится прямоугольник, одна стророна которого лежит на ОХ, а другая пересекает график подынтегральной функции. Сумма площадей полученных ступеней будет приближенной оценкой площади. Для решения необходимо найти шаг разбиения $h = \frac{b-a}{n}$. Высоты определяются так:

1. Левые квадраты - левая сторона прямоугольника, $x = x_i$, $i \in \{ 0...n-1\}$;
2. Правые квадраты - правая сторона прямоугольника, $x = x_i$, $i \in \{ 1...n\}$;
3. Средние квадраты - середина между сторонами, $x = x_i + \frac{h}{2}$, $i \in \{0...n-1\}$

$$
S = h \Sigma f(x_i)
$$

![[Pasted image 20251217233726.png]]
Погрешность можно расчитать из нахождения модуля разницы приближений:
$|I_{2n}-I_n|$.
Метод, основанный на применении правила Рунге, для _средних_ прямоугольников даёт $\frac{1}{3}|I_{2n}-I_n|$

## 7. Формула трапеции. Геометрический смысл. Погрешность.

Идея состоит в том, чтобы разбить промежуток интегрирования на несколько отрезков, где график подынтегральной функции приближается к ломаной линии.
![[Pasted image 20251217234904.png]]
Метод даёт большее приближение, чем метод прямоугольников.

$$
\int_{a}^{b}f(x)dx \approx h \Big(\frac{f(x_0)-f(x_n)}{2}+f(x_1)+...f(x_{n-1})\Big)
$$

По сути сумма лощадей $n$ трапеций.
Погрешность можно расчитать из нахождения модуля разницы приближений:
$|I_{2n}-I_n|$.
Метод, основанный на применении правила Рунге, даёт $\frac{1}{3}|I_{2n}-I_n|$.
Если по Ругле абсолютная разница меньше погрешности, то нет необходимости увеличивать количество точек.

## 8. Формула Симпсона. Геометрический смысл. Погрешность.

Метод состоит в том, чтобы разделить подынтегральный график функции на некоторое **чётное** количестро отрезков, которые будут приближены параболами.

$$
\int_{a}^{b}f(x)dx \approx \frac{h}{3} \Big( f(x_0)+f(x_{2n})+2(f(x_2)+f(x_4)+\dots+f(x_{2n-2})+4(f(x_1)+\dots+f(x_{2n-1}))\Big)
$$

где $h = \frac{b-a}{2n}$.
$f(x_0) + f(x_{2n})$ – сумма первого и последнего значения подынтегральной функции;
$2(f(x_2)+f(x_4)+\dots+f(x_{2n-2})$ – сумма членов с _чётными_ индексами умножается на 2;
![[Pasted image 20251218000849.png]]
$4(f(x_1)+\dots+f(x_{2n-1}))$ – сумма членов с _нечётными_ индексами умножается на 4.

$$
E = \frac{h^5f^{(4)}(\xi)}{180}
$$

## 9. Формула 3/8. Погрешность.

Метод состоит в том, чтобы разделить подынтегральный график функции на некоторое количестро отрезков кратное 3, которые будут приближены параболами.

$$
\int_{a}^{b}f(x)dx \approx \frac{3h}{8} \Big( f(x_0)+f(x_{2n})+3(f(x_1) + f(x_2)+f(x_4)+\dots)+2(f(x_3)+f(x_6)+\dots))\Big)
$$

где $h = \frac{b-a}{2n}$.
$f(x_0) + f(x_{2n})$ – сумма первого и последнего значения подынтегральной функции;
$2(f(x_3)+f(x_6)+\dots)$ – кратные 3 умножаются на 2;
$3(f(x_1) + f(x_2)+f(x_4)+\dots)$ – сумма сумма остальных членов умножается на 3.
![[Pasted image 20251218000849.png]]
$$E = -\frac{3h^5f^{(4)}(\xi)}{80}$$

## 10. Нормы векторов и матриц. Определения и свойства. Согласованность норм.

### Понятие нормы

Норма — это числовая характеристика величины вектора или матрицы, позволяющая количественно оценивать размеры ошибок и устойчивость алгоритмов.

### Нормы векторов

Функция $\|x\|$ называется нормой, если выполняются аксиомы:

1. $\|x\|\ge 0$, $\|x\|=0 \iff x=0$
2. $\|\alpha x\|=|\alpha|\|x\|$
3. $$\|x+y\|\le \|x\|+\|y\|$$

Основные нормы:

-   **Евклидова норма**
    $$
    \|x\|_2=\sqrt{\sum_{i=1}^n x_i^2}
    $$
-   **Норма суммы модулей**
    $$
    \|x\|_1=\sum_{i=1}^n |x_i|
    $$
-   **Максимальная норма**
    $$
    \|x\|_\infty=\max_i |x_i|
    $$

### Нормы матриц

Норма матрицы $A$ называется согласованной с нормой вектора, если
$$
\|Ax\|\le \|A\|\|x\| \quad \forall x
$$

Примеры согласованных норм:
$$
\|A\|_1=\max_j \sum_i |a_{ij}|, \quad
\|A\|_\infty=\max_i \sum_j |a_{ij}|
$$

### Значение норм

Нормы используются для:

-   оценки погрешностей
-   анализа сходимости итерационных методов
-   определения обусловленности задач

## 11. Устойчивость СЛАУ. Обусловленность СЛАУ.

Обусловленность - характеристика матрицы СЛАУ, которая показывает насколько чувствительно решение к входным данным.
Устойчивость - характеристика метода для решения СЛАУ, которая показывает как сильно влияет решение на уже имеющиеся ошибки.

## 12. Метод простых итераций. Сходимость.

Рассмотрим систему линейных алгебраических уравнений:
$$
Ax=b
$$

Метод простых итераций основан на приведении системы к эквивалентному виду:
$$
x=Bx+c
$$

### Итерационный процесс

$$
x^{(k+1)}=Bx^{(k)}+c
$$

Начальное приближение $x^{(0)}$ выбирается произвольно.

### Условие сходимости

Метод сходится тогда и только тогда, когда спектральный радиус матрицы $B$ удовлетворяет условию:
$$
\rho(B)<1
$$

Практически используется достаточное условие:
$$
\|B\|<1
$$

### Характер сходимости

-   сходимость линейная
-   скорость определяется величиной $\|B\|$
-   чувствителен к плохой обусловленности матрицы

## 13. Метод Зейделя. Случай нормальной системы. Сходимость.

Метод Зейделя (метод последовательных приближений с использованием новых значений) предназначен для численного решения систем линейных алгебраических уравнений
$$
Ax=b.
$$
Он является развитием метода простых итераций и отличается более быстрой сходимостью за счёт использования уже вычисленных значений неизвестных на текущей итерации.

### Идея метода

В методе простых итераций новое приближение полностью вычисляется по значениям предыдущего шага. В методе Зейделя компоненты вектора решения обновляются **последовательно**, и каждое новое значение сразу используется в дальнейших вычислениях текущей итерации.

Это эквивалентно переходу от одновременной коррекции всех компонент к пошаговой коррекции, что существенно ускоряет процесс.

### Вывод итерационной формулы

Разложим матрицу системы на сумму:
$$
A=D+L+U,
$$
где:

-   $D$ — диагональная матрица,
-   $L$ — нижняя треугольная часть,
-   $U$ — верхняя треугольная часть.

Исходная система принимает вид:
$$
(D+L)x=-Ux+b.
$$
Отсюда итерационный процесс метода Зейделя:
$$
x^{(k+1)}=(D+L)^{-1}(b-Ux^{(k)}).
$$

В покомпонентной форме:
$$
x*i^{(k+1)}=\frac{1}{a_{ii}}\Big(b*i-\sum_{j<i}a_{ij}x_j^{(k+1)}-\sum_{j>i}a_{ij}x_j^{(k)}\Big).
$$

### Сходимость метода

Метод Зейделя сходится при выполнении хотя бы одного из условий:

-   матрица диагонально преобладающая;
-   система является нормальной.

### Нормальная система

Система называется **нормальной**, если:
$$
A=A^T, \quad (Ax,x)>0 \; \forall x\neq0.
$$
Для нормальных систем метод Зейделя **гарантированно сходится** при любом начальном приближении.

### Свойства метода

-   сходимость быстрее, чем у метода простых итераций;
-   простая реализация;
-   чувствительность к перестановке уравнений;
-   широко применяется для разреженных систем.

## 14. Каноническая форма записи одношаговых итерационных методов. Примеры.

Одношаговые итерационные методы используются для приближённого решения СЛАУ и основаны на последовательном уточнении приближения решения.

### Каноническая форма

Общий вид одношагового итерационного метода записывается как:
$$
x^{(k+1)}=x^{(k)}+\tau_k r^{(k)},
$$
где:
$$
r^{(k)}=b-Ax^{(k)}
$$
— вектор невязки.

### Геометрический смысл

Каждый шаг корректирует текущее приближение в направлении невязки. Параметр $\tau_k$ определяет длину шага и напрямую влияет на скорость сходимости.

### Примеры одношаговых методов

1. **Метод простых итераций** — $\tau_k=\tau=const$.
2. **Метод минимальных невязок** — минимизируется норма $r^{(k+1)}$.
3. **Метод наискорейшего спуска** — минимизируется квадратичный функционал.

### Особенности

-   универсальность формы;
-   возможность оптимального выбора параметра;
-   чувствительность к обусловленности матрицы.

## 15. Вариационные методы решения СЛАУ. Выбор оптимального параметра.

Для симметричных положительно определённых матриц задача решения СЛАУ может быть сведена к задаче минимизации функционала.

### Вариационная постановка

Рассмотрим функционал:
$$
\Phi(x)=\frac{1}{2}(Ax,x)-(b,x).
$$
Минимум функционала достигается в точке, где:
$$
\nabla \Phi(x)=Ax-b=0.
$$

### Метод наискорейшего спуска

Итерационный процесс:
$$
x^{(k+1)}=x^{(k)}+\tau_k r^{(k)},
$$
где $r^{(k)}=b-Ax^{(k)}$.

### Оптимальный параметр

Параметр выбирается из условия минимума функционала:
$$
\tau_k=\frac{(r^{(k)},r^{(k)})}{(Ar^{(k)},r^{(k)})}.
$$

### Свойства метода

-   гарантированная сходимость;
-   линейная скорость сходимости;
-   высокая устойчивость.

## 16. Метод прогонки решения СЛАУ. Условия устойчивости прогонки.

Метод прогонки применяется для решения систем линейных уравнений с трёхдиагональной матрицей:
$$
a*ix_{i-1}+b*ix_i+c_ix_{i+1}=d_i.
$$

### Идея метода

Метод основан на последовательном исключении неизвестных и состоит из двух этапов:

1. прямой ход;
2. обратный ход.

### Прямой ход

Вводятся коэффициенты прогонки:
$$
\alpha*i=-\frac{c_i}{b_i+a_i\alpha_{i-1}}, \quad
\beta*i=\frac{d_i-a_i\beta_{i-1}}{b*i+a_i\alpha_{i-1}}.
$$

### Обратный ход

$$
x*n=\beta_n, \quad x_i=\alpha_i x_{i+1}+\beta_i.
$$

### Условия устойчивости

Достаточное условие устойчивости:
$$
|b_i|\ge |a_i|+|c_i|.
$$

Метод обладает линейной вычислительной сложностью.

## 17. Обращение матриц. Метод окаймления.

Задача обращения матрицы заключается в нахождении такой матрицы $A^{-1}$, что:
$$
AA^{-1}=I.
$$

### Метод окаймления

Метод окаймления позволяет находить обратную матрицу поэтапно, добавляя строки и столбцы.

Пусть известна $A*k^{-1}$. Рассмотрим расширенную матрицу:
$$
A_{k+1}=\begin{pmatrix}
A_k & u \\
v^T & \alpha
\end{pmatrix}.
$$

Обратная матрица вычисляется рекуррентно, что существенно снижает вычислительные затраты.

### Преимущества

-   экономия вычислений;
-   удобство при адаптивных задачах;
-   хорошая численная устойчивость.

## 18. Метод разложения в произведение двух треугольных матриц.

Метод основан на представлении матрицы системы в виде:
$$
A=LU,
$$
где $L$ — нижняя, а $U$ — верхняя треугольные матрицы.

### Алгоритм метода

1. Выполняется LU-разложение матрицы $A$.
2. Решается система $Ly=b$.
3. Решается система $Ux=y$.

### Свойства метода

-   высокая эффективность;
-   удобство при многократных решениях;
-   необходимость выбора главного элемента для устойчивости.

## 19. Решение СЛАУ методом главных элементов.

При решении систем линейных алгебраических уравнений методом Гаусса существенную роль играет выбор ведущего элемента. Если этот элемент мал по модулю, то ошибки округления могут значительно исказить результат. Для повышения численной устойчивости применяется **метод главных элементов**.

### Идея метода

Основная идея состоит в том, чтобы на каждом шаге исключения выбирать в качестве ведущего (опорного) элемента **наибольший по модулю элемент** из допустимой части матрицы.

Различают:

-   выбор главного элемента по столбцу (частичный выбор);
-   выбор главного элемента по всей подматрице (полный выбор).

### Алгоритм метода

Рассмотрим расширенную матрицу системы $[A|b]$.

На $k$-м шаге:

1. В подматрице $a_{ij},\ i,j\ge k$ выбирается элемент с максимальным модулем.
2. Выполняются перестановки строк (и при полном выборе — столбцов).
3. Проводится обычный шаг метода Гаусса.

### Математическое обоснование

Выбор главного элемента позволяет уменьшить коэффициенты роста и, следовательно, снизить накопление ошибок округления.

### Свойства метода

-   существенно повышает устойчивость;
-   увеличивает вычислительные затраты;
-   рекомендуется для плохо обусловленных систем.

## 20. Метод квадратных корней.

Метод квадратных корней (метод Холецкого) применяется для решения СЛАУ с симметричной положительно определённой матрицей.

### Постановка задачи

Пусть:
$$
A=A^T, \quad (Ax,x)>0.
$$

Тогда матрицу можно представить в виде:
$$
A=LL^T,
$$
где $L$ — нижняя треугольная матрица.

### Алгоритм метода

1. Выполняется разложение Холецкого:
   $$
   a_{ij}=\sum_{k=1}^{j}l_{ik}l_{jk}.
   $$
2. Решается система:
   $$
   Ly=b.
   $$
3. Решается система:
   $$
   L^Tx=y.
   $$

### Преимущества метода

-   высокая численная устойчивость;
-   экономия операций по сравнению с LU-разложением;
-   отсутствие необходимости перестановок.

### Ограничения

Метод применим **только** для симметричных положительно определённых матриц.

## 21. Методы отделения корней нелинейных уравнений.

При решении нелинейных уравнений
$$
f(x)=0
$$
важно предварительно определить интервалы, содержащие корни. Эта задача называется **отделением корней**.

### Основная идея

Если функция непрерывна на отрезке $[a,b]$ и
$$
f(a)f(b)<0,
$$
то на этом отрезке существует по крайней мере один корень.

### Методы отделения корней

1. **Табулирование** — вычисление значений функции в узлах сетки.
2. **Графический метод** — анализ графика функции.
3. **Анализ производной** — поиск интервалов монотонности.

### Назначение этапа

Отделение корней необходимо для:

-   гарантированной сходимости итерационных методов;
-   исключения кратных корней;
-   повышения надёжности вычислений.

## 22. Метод половинного деления.

Метод половинного деления (бисекции) — один из самых надёжных методов решения нелинейных уравнений.

### Алгоритм метода

Пусть корень заключён в отрезке $[a,b]$, где
$$
f(a)f(b)<0.
$$

На каждом шаге:
$$
x_k=\frac{a_k+b_k}{2}.
$$

Интервал выбирается заново в зависимости от знака $f(x_k)$.

### Сходимость метода

Метод всегда сходится при выполнении условия смены знака.

Скорость сходимости — линейная:
$$
|x_k-x^*|\le \frac{b-a}{2^k}.
$$


### Достоинства и недостатки
-   абсолютная надёжность;
    − медленная сходимость.

## 23. Метод хорд. Геометрический смысл.

Метод хорд является усовершенствованием метода половинного деления и использует линейную аппроксимацию функции.

### Итерационная формула

Пусть заданы две точки $x_{k-1}$ и $x_k$:
$$
x_{k+1}=x*k-\frac{f(x_k)(x_k-x_{k-1})}{f(x*k)-f(x_{k-1})}.
$$

### Геометрический смысл

На каждом шаге проводится хорда (секущая) графика функции, и точка пересечения этой хорды с осью $Ox$ принимается за следующее приближение корня.

### Свойства метода

-   скорость сходимости выше, чем у бисекции;
-   не требует вычисления производной;
-   возможна потеря сходимости при плохом выборе начальных точек.

## 24. Метод касательных (Ньютона). Геометрический смысл. Особенности.

Метод Ньютона — один из наиболее эффективных методов решения нелинейных уравнений.

### Итерационная формула

$$
x_{k+1}=x_k-\frac{f(x_k)}{f'(x_k)}.
$$

### Геометрический смысл

На каждом шаге проводится касательная к графику функции в точке $x_k$. Точка пересечения касательной с осью $Ox$ даёт следующее приближение корня.

### Сходимость метода

При достаточно хорошем начальном приближении метод обладает **квадратичной сходимостью**:
$$
|x_{k+1}-x|\approx C|x_k-x|^2.
$$

### Особенности метода

-   высокая скорость сходимости;
-   необходимость вычисления производной;
-   возможна расходимость при плохом начальном приближении.

## 25. Модификации метода Ньютона.

Классический метод Ньютона обладает высокой (квадратичной) скоростью сходимости, однако на практике его применение может быть затруднено из-за необходимости вычисления производной или чувствительности к начальному приближению. Поэтому используются различные модификации метода.

### Метод Ньютона с постоянной производной

Производная вычисляется один раз в начальной точке:
$$
x_{k+1}=x_k-\frac{f(x_k)}{f'(x_0)}.
$$

Метод становится линейно сходящимся, но значительно упрощается вычислительно.

### Метод секущих

Производная аппроксимируется конечной разностью:
$$
x_{k+1}=x_k-\frac{f(x_k)(x_k-x_{k-1})}{f(x*k)-f(x_{k-1})}.
$$

Метод не требует вычисления производной и имеет порядок сходимости примерно 1.618.

### Упрощённый метод Ньютона

Производная пересчитывается не на каждом шаге, а периодически. Это компромисс между скоростью и устойчивостью.

### Сравнительные свойства

-   классический Ньютон — максимальная скорость;
-   секущие — меньше вычислений;
-   упрощённый — баланс устойчивости и точности.

## 26. Итерационные методы нахождения собственных значений матриц.

Задача нахождения собственных значений матрицы:
$$
Ax=\lambda x
$$
является ключевой во многих прикладных задачах механики, физики и численного анализа.

### Общая идея итерационных методов

Вместо прямого вычисления характеристического многочлена используются итерационные процессы, сходящиеся к собственным значениям и векторам.

### Основные требования

-   матрица конечного размера;
-   наличие доминирующих собственных значений;
-   корректный выбор начального вектора.

## 27. Степенной метод. Сходимость и свойства.

Степенной метод применяется для нахождения собственного значения матрицы, максимального по модулю.

### Итерационный процесс

Пусть выбран ненулевой начальный вектор $x^{(0)}$. Тогда:
$$
x^{(k+1)}=Ax^{(k)}, \quad y^{(k+1)}=\frac{x^{(k+1)}}{\|x^{(k+1)}\|}.
$$

Собственное значение оценивается по формуле:
$$
\lambda^{(k)}=\frac{(Ax^{(k)},x^{(k)})}{(x^{(k)},x^{(k)})}.
$$

### Сходимость

Метод сходится, если существует единственное доминирующее собственное значение:
$$
|\lambda_1|>|\lambda_2|.
$$

Скорость сходимости определяется отношением:
$$
\left|\frac{\lambda_2}{\lambda_1}\right|.
$$

### Достоинства и недостатки

-   простота реализации;
    − невозможность нахождения остальных собственных значений.

## 28. Обратный степенной метод.

Обратный степенной метод используется для нахождения собственного значения, минимального по модулю.

### Идея метода

Применяется степенной метод к матрице $A^{-1}$:
$$
x^{(k+1)}=A^{-1}x^{(k)}.
$$

На практике решается система:
$$
Ay=x^{(k)}.
$$

### Свойства

-   позволяет находить малые собственные значения;
-   требует решения СЛАУ на каждом шаге;
-   чувствителен к погрешностям решения.

## 29. Метод сдвига для нахождения собственных значений.

Для нахождения собственных значений, близких к заданному числу $\mu$, используется метод сдвига.

### Итерационный процесс

Рассматривается матрица:
$$
(A-\mu I)^{-1}.
$$

Применение степенного метода к этой матрице позволяет получить собственное значение, ближайшее к $\mu$.

### Преимущества

-   высокая точность;
-   гибкость выбора интересующего спектра.

## 30. Погрешности при вычислении собственных значений.

Вычисление собственных значений является чувствительной задачей, особенно для плохо обусловленных матриц.

### Источники погрешностей

-   округления при арифметических операциях;
-   ошибки решения СЛАУ;
-   плохая обусловленность матрицы.

### Оценка устойчивости

Малые возмущения матрицы могут приводить к значительным изменениям спектра:
$$
|\delta \lambda|\le \|\delta A\|.
$$

### Практические рекомендации

-   использовать ортогональные преобразования;
-   избегать прямого вычисления характеристического многочлена;
-   применять устойчивые итерационные методы.

## 31. Задача Коши для обыкновенных дифференциальных уравнений.

Задача Коши для обыкновенного дифференциального уравнения первого порядка формулируется следующим образом:
$$
y'(x)=f(x,y), \quad y(x_0)=y_0.
$$

### Смысл задачи

Требуется найти функцию $y(x)$, удовлетворяющую дифференциальному уравнению и начальному условию. В прикладных задачах аналитическое решение удаётся получить редко, поэтому используются численные методы.

### Существование и единственность решения

Если функция $f(x,y)$ непрерывна и удовлетворяет условию Липшица по $y$ в окрестности точки $(x_0,y_0)$, то задача Коши имеет единственное решение.

### Численная постановка

Рассматривается сетка:
$$
x_n=x_0+nh, \quad n=0,1,2,\dots
$$
где $h$ — шаг интегрирования.

## 32. Метод Эйлера. Погрешность и устойчивость.

Метод Эйлера является простейшим численным методом решения задачи Коши.

### Вывод метода

Разложим решение в ряд Тейлора:
$$
y(x_{n+1})=y(x_n)+hy'(x_n)+O(h^2).
$$
Подставляя $y'(x_n)=f(x_n,y_n)$, получаем итерационную формулу:
$$
y_{n+1}=y_n+h f(x_n,y_n).
$$

### Погрешности

-   локальная погрешность: $O(h^2)$;
-   глобальная погрешность: $O(h)$.

### Устойчивость

Метод условно устойчив и плохо подходит для жёстких задач.

## 33. Улучшенный метод Эйлера (метод Эйлера–Коши).

Для повышения точности используется модификация метода Эйлера.

### Алгоритм метода

1. Предсказание:
   $$
   \tilde y_{n+1}=y_n+h f(x_n,y_n).
   $$
2. Исправление:
   $$
   y_{n+1}=y_n+\frac{h}{2}\left[f(x_n,y_n)+f(x_{n+1},\tilde y_{n+1})\right].
   $$

### Точность метода

Глобальная погрешность порядка $O(h^2)$.

### Свойства

-   повышенная точность;
-   незначительное увеличение вычислений;
-   всё ещё ограниченная устойчивость.

## 34. Методы Рунге–Кутты.

Методы Рунге–Кутты представляют собой класс одношаговых методов высокого порядка точности.

### Метод Рунге–Кутты четвёртого порядка

Итерационная схема:
$$
k_1=f(x_n,y_n),
$$
$$
k_2=f\left(x_n+\frac{h}{2},y_n+\frac{h}{2}k_1\right),
$$
$$
k_3=f\left(x_n+\frac{h}{2},y_n+\frac{h}{2}k_2\right),
$$
$$
k_4=f(x_n+h,y_n+hk_3).
$$

$$
y_{n+1}=y_n+\frac{h}{6}(k_1+2k_2+2k_3+k_4).
$$

### Свойства метода

-   высокая точность;
-   хорошая устойчивость;
-   отсутствие необходимости вычислять производные.

## 35. Многошаговые методы. Общая идея.

Многошаговые методы используют значения решения на нескольких предыдущих шагах.

### Общий вид

$$
\sum_{j=0}^{k} \alpha_j y_{n+j}=h\sum_{j=0}^{k}\beta_j f_{n+j}.
$$

### Особенности

-   высокая эффективность;
-   необходимость стартовых значений;
-   чувствительность к ошибкам начальных шагов.

## 36. Явные и неявные методы. Устойчивость.

### Явные методы

Новое значение выражается явно:
$$
y_{n+1}=y_n+h f(x_n,y_n).
$$

### Неявные методы

Новое значение определяется из уравнения:
$$
y_{n+1}=y_n+h f(x_{n+1},y_{n+1}).
$$

### Сравнение

-   явные методы проще;
-   неявные методы устойчивее;
-   неявные предпочтительны для жёстких задач.

## 37. Понятие жёсткости задачи.

Задача Коши называется **жёсткой**, если при выборе шага, обеспечивающего устойчивость, точность оказывается избыточной.

### Причины жёсткости

-   наличие быстро затухающих компонент;
-   сильно различающиеся собственные значения.

### Последствия

Требуются специальные устойчивые методы.

## 38. Численные методы решения жёстких задач.

Для решения жёстких задач применяются неявные методы:

-   неявный метод Эйлера;
-   метод трапеций;
-   методы Розенброка.

### Неявный метод Эйлера

$$
y_{n+1}=y_n+h f(x_{n+1},y_{n+1}).
$$

### Свойства

-   абсолютная устойчивость;
-   необходимость решения нелинейных уравнений на каждом шаге;
-   высокая надёжность.
