### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №1

**1. Директивы OpenMP. Директивы синхронизации и взаимодействия потоков во времени**

* **Теория:**
В OpenMP потоки выполняются параллельно, но иногда им нужно "договориться" между собой (синхронизироваться). Основные директивы:
* `#pragma omp barrier`: Точка сбора. Все потоки останавливаются здесь и ждут, пока последний поток не дойдет до этой точки, только потом продолжают выполнение.
* `#pragma omp critical`: Критическая секция. Код внутри этого блока может выполнять только **один** поток одновременно. Используется для защиты общих переменных от "гонки данных".
* `#pragma omp atomic`: Облегченная версия critical, работает только для простых операций обновления памяти (например, `x++` или `x += y`).
* `#pragma omp master`: Блок выполняет только главный поток (master, поток №0), остальные пропускают.


* **Пример (C++):**

```cpp
#include <omp.h>
#include <stdio.h>

int main() {
    int sum = 0;
    #pragma omp parallel
    {
        // Каждый поток делает что-то свое...
        
        // Синхронизация: ждем всех
        #pragma omp barrier 

        // Безопасное обновление общей переменной (только 1 поток одновременно)
        #pragma omp critical
        {
            sum += 1;
            printf("Поток %d увеличил сумму\n", omp_get_thread_num());
        }
    }
    printf("Итоговая сумма: %d\n", sum);
    return 0;
}

```

**2. Параллельные вычислительные системы**

* **Теория:**
Это компьютерные системы, использующие множество процессоров/ядер для одновременного решения одной задачи.
Классификация Флинна:
1. **SISD:** Одиночный поток команд, одиночный поток данных (обычный старый ПК).
2. **SIMD:** Одиночный поток команд, множественный поток данных (векторные инструкции, GPU). Все процессоры делают одну команду (например, "сложить"), но с разными данными.
3. **MIMD:** Множественный поток команд, множественный поток данных (кластеры, многоядерные CPU). Каждый процессор делает свою уникальную работу.
*Типы памяти:* Системы с общей памятью (Shared Memory — SMP) и с распределенной памятью (Distributed Memory — Кластеры).



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №2

**1. Выявления зависимости результатов вычислений от размещения переменных в памяти (на примере умножения матриц)**

* **Теория:**
В языке C/C++ двумерные массивы хранятся в памяти **построчно**. Это значит, что элементы `A[0][0]` и `A[0][1]` лежат рядом (попадают в один кэш-линию процессора), а `A[0][0]` и `A[1][0]` — далеко друг от друга.
*Правильный доступ (Locality of reference):* Идти по памяти последовательно.
*Неправильный доступ:* "Прыгать" по памяти. Это вызывает **Cache Misses** (промахи кэша) и сильно замедляет программу.
При умножении матриц классический цикл `i-j-k` может быть медленным, если внутренняя итерация идет по столбцу. Оптимизация — перестановка циклов (`i-k-j`).
* **Пример (C++):**

```cpp
// Плохой способ (много промахов кэша для matrixB)
for (int i = 0; i < N; i++)
    for (int j = 0; j < N; j++)
        for (int k = 0; k < N; k++)
            res[i][j] += matrixA[i][k] * matrixB[k][j]; // B скачет по памяти

// Хороший способ (оптимизирован под кэш)
for (int i = 0; i < N; i++)
    for (int k = 0; k < N; k++) // Поменяли местами j и k
        for (int j = 0; j < N; j++)
            res[i][j] += matrixA[i][k] * matrixB[k][j]; // Теперь B читаем последовательно

```

**2. Принципы разработки параллельных алгоритмов**

* **Теория:**
Методология Фостера (PCAM):
1. **Декомпозиция (Partitioning):** Разбить задачу на множество мелких подзадач (кусочков данных или функций).
2. **Коммуникация (Communication):** Определить, как подзадачи должны обмениваться данными (пересылка сообщений).
3. **Агломерация (Agglomeration):** Объединение мелких подзадач в более крупные блоки, чтобы накладные расходы на передачу данных не превышали выгоду от параллелизма.
4. **Отображение (Mapping):** Распределение укрупненных задач по реальным процессорам для балансировки нагрузки.



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №3

**1. Большие задачи. Вычислительная техника и технологии для больших задач**

* **Теория:**
"Большие задачи" (Grand Challenges) — это задачи, требующие колоссальных вычислительных ресурсов, которые невозможно решить на обычном ПК за разумное время.
*Примеры:* Прогноз погоды/климата, моделирование сворачивания белков, ядерные реакции, анализ Big Data.
*Технологии:*
* **Суперкомпьютеры:** (TOP500) — тысячи узлов, соединенных быстрой сетью (InfiniBand).
* **Грид-системы (Grid):** Объединение географически распределенных компьютеров (например, через интернет) для одной задачи.
* **Облачные вычисления (Cloud):** Аренда мощностей (AWS, Azure) по требованию.



**2. Теоретические основы параллельных вычислений (Законы ускорения)**

* **Теория:**
* **Ускорение (Speedup, ):** Во сколько раз параллельная программа быстрее последовательной. 

, где  — время на 1 проце,  — на  процах.
* **Закон Амдала:** Ограничивает максимальное ускорение. Если в программе есть доля кода , которую *нельзя* распараллелить (строго последовательная), то даже с бесконечным числом процессоров ускорение не превысит .
* **Эффективность ():** Показывает, насколько полезно загружены процессоры. 

.



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №4

**1. Оценка трудоемкости вычислительных задач**

* **Теория:**
Трудоемкость оценивается количеством элементарных операций (сложение, умножение), необходимых для решения задачи. Обозначается через .
В параллельном программировании добавляется:
1. **Вычислительная сложность:** Время на расчеты.
2. **Коммуникационная сложность:** Время на пересылку данных между узлами. Это самое "узкое" место. Пересылка данных в тысячи раз медленнее вычислений.
*Цель:* Максимизировать вычисления и минимизировать обмены данными.



**2. Системы параллельного программирования**

* **Теория:**
Это набор языков, библиотек и инструментов для написания параллельного кода.
1. **Для общей памяти (Shared Memory):**
* **OpenMP:** Директивы компилятора (C++, Fortran). Простота использования.
* **Pthreads:** Низкоуровневые потоки POSIX. Сложно, но гибко.


2. **Для распределенной памяти (Distributed Memory):**
* **MPI (Message Passing Interface):** Стандарт де-факто для кластеров. Явный обмен сообщениями (`Send`/`Recv`).


3. **Гибридные:** MPI + OpenMP (MPI между узлами, OpenMP внутри узла).
4. **GPU:** CUDA, OpenCL.



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №5

**1. Обзор средств программирования вычислительных систем. Система и стандарты OpenMP**

* **Теория:**
OpenMP (Open Multi-Processing) — это API для написания многопоточных приложений на системах с **общей памятью** (многоядерные процессоры).
*Состав:*
1. **Директивы компилятора** (`#pragma omp ...`) — указывают, где параллелить.
2. **Библиотечные функции** (`omp_get_thread_num()`) — управление в runtime.
3. **Переменные окружения** (`OMP_NUM_THREADS`) — настройка без перекомпиляции.
*Принцип:* Модель Fork-Join. Программа начинается как один поток, при встрече `#pragma omp parallel` "разветвляется" (fork) на группу потоков, а в конце параллельного блока сливается обратно (join).


* **Пример (Hello World):**

```cpp
#pragma omp parallel
{
    printf("Hello form thread\n");
}

```

**2. Технология MPI**

* **Теория:**
MPI (Message Passing Interface) — стандарт для систем с **распределенной памятью** (кластеры).
*Суть:* Запускается  независимых процессов (обычно по 1 на ядро). У каждого процесса *своя* память. Процесс А не может просто прочитать переменную процесса Б.
Чтобы передать данные, процесс А должен явно сделать `Send`, а процесс Б — `Recv`.
*Базовая структура MPI программы:*
1. `MPI_Init` — начало.
2. `MPI_Comm_rank` — "кто я?" (мой номер).
3. `MPI_Comm_size` — "сколько нас?".
4. Работа и обмен сообщениями.
5. `MPI_Finalize` — конец.


* **Пример (C++):**

```cpp
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv); // Инициализация

    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Получить мой номер

    printf("Привет, я процесс %d\n", rank);

    MPI_Finalize(); // Завершение
    return 0;
}

```

---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №6

**1. Задание опций компилятора. Определение параллельного региона и параллелизация циклов (OpenMP)**

* **Теория:**
* **Компиляция:** Чтобы OpenMP заработал, нужно сказать об этом компилятору.
* GCC (Linux/MinGW): флаг `-fopenmp` (пример: `gcc -fopenmp main.c -o app`).
* MSVC (Visual Studio): В настройках проекта включить "OpenMP Support" (`/openmp`).


* **Параллелизация циклов:** Самая частая задача. Директива `#pragma omp parallel for` автоматически делит итерации цикла между потоками.
* **Распределение (Schedule):**
* `static`: Итерации делятся поровну заранее (быстро, но плохо, если одна итерация тяжелее другой).
* `dynamic`: Потоки берут порции работы по мере освобождения (хорошая балансировка, но накладные расходы).




* **Пример (C++):**

```cpp
#include <omp.h>
#include <stdio.h>

int main() {
    int N = 1000;
    int a[1000], b[1000], c[1000];

    // Инициализация массивов
    for(int i=0; i<N; i++) { a[i] = i; b[i] = i*2; }

    // Автоматическое распараллеливание цикла
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < N; i++) {
        c[i] = a[i] + b[i]; // Каждый поток считает свой кусок i
    }
    
    printf("Готово!\n");
    return 0;
}

```

**2. Типовые модели программирования и шаблоны**

* **Теория:**
Как организовать параллельную работу?
1. **SPMD (Single Program Multiple Data):** Самая популярная модель (MPI). Одна и та же программа запускается на всех узлах, но обрабатывает разные данные (зависит от `rank`).
2. **Master-Slave (Manager-Worker):** Один главный процесс (Master) раздает задачи, остальные (Slaves) их выполняют и возвращают результат. Хорошо для балансировки нагрузки.
3. **Конвейер (Pipeline):** Процесс 1 делает первый этап, передает Процессу 2, тот делает второй этап и т.д. Как на заводе.
4. **Разделяй и властвуй (Divide and Conquer):** Рекурсивное разбиение задачи на подзадачи (дерево процессов).



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №7

**1. Измерение коммуникационных параметров кластерной системы в условиях синхронизации**

* **Теория:**
Коммуникация (передача данных) характеризуется двумя параметрами:
1. **Латентность (Latency):** Время задержки. Сколько времени нужно, чтобы первый бит данных просто долетел до получателя (пинг).
2. **Пропускная способность (Bandwidth):** Скорость передачи (Гб/сек).
*При синхронизации:* Если используется `MPI_Barrier` или блокирующие обмены (`Send/Recv`), время выполнения программы определяется **самым медленным** узлом. Измеряют время "простоя" (Idle time) — сколько времени быстрые процессоры ждут медленных на барьере. Используют `MPI_Wtime()`.



**2. Технология программирования MapReduce**

* **Теория:**
Модель от Google для обработки петабайт данных на тысячах серверов. Скрывает от программиста сложности сети и отказов оборудования.
Состоит из двух этапов:
1. **Map (Отображение):** Главный узел "нарезает" входной файл на куски. Рабочие узлы обрабатывают свои куски и выдают пары `ключ: значение`. (Например, подсчет слов: `cat: 1`, `dog: 1`).
2. **Shuffle (Тасование):** Система автоматически сортирует данные так, чтобы все одинаковые ключи попали на один узел.
3. **Reduce (Свертка):** Узел получает список значений для ключа и "сворачивает" их в итог (Суммирует: `cat: [1, 1, 1] -> cat: 3`).



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №8

**1. Измерение коммуникационных параметров кластерной системы в условиях блокировки**

* **Теория:**
*Блокировка* означает, что процесс останавливается и ждет завершения операции.
* `MPI_Send` блокируется, пока данные не скопированы в буфер (или не переданы).
* `MPI_Recv` блокируется, пока сообщение не придет.
Если процессы тратят 50% времени на блокировку в `MPI_Recv`, значит, сеть медленная или плохая балансировка (одни работают, другие ждут).
Измеряется через замеры `t1 = MPI_Wtime()` до вызова и `t2 = MPI_Wtime()` после. Разница — потери времени.



**2. Приемы и стратегии реализации MapReduce-программ**

* **Теория:**
Главный принцип: **Передавай код к данным, а не данные к коду**.
*Стратегии оптимизации:*
1. **Локальность данных (Data Locality):** Запускать Map-задачу на том же физическом сервере (или жестком диске), где лежит кусочек файла. Чтобы не гонять гигабайты по сети.
2. **Combiner (Локальный Reduce):** Предварительное сжатие данных на этапе Map.
* *Без Combiner:* Map отправляет по сети `("apple", 1), ("apple", 1), ("apple", 1)`.
* *С Combiner:* Map сам сложит и отправит `("apple", 3)`. Снижает нагрузку на сеть.


3. **Спекулятивное выполнение:** Если один узел тормозит, мастер запускает ту же задачу на другом узле. Кто первый закончил — того результат и берем.



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №9

**1. Измерение производительности параллельных алгоритмов в зависимости от параметров параллелизации**

* **Теория:**
Исследуется **Масштабируемость (Scalability)**.
Строят график зависимости Ускорения () от числа процессоров ().
* **Сильная масштабируемость (Strong Scaling):** Размер задачи фиксирован, увеличиваем число процессоров. Цель — решить задачу быстрее. В какой-то момент ускорение перестает расти (закон Амдала).
* **Слабая масштабируемость (Weak Scaling):** Увеличиваем число процессоров, но и размер задачи увеличиваем пропорционально. Цель — решить *бóльшую* задачу за то же время.



**2. MapReduce. Инструменты и практическое применение**

* **Теория:**
Где применяется: Поисковая индексация (Google), анализ логов, рекомендательные системы (Netflix), сортировка петабайтов данных.
*Инструменты:*
1. **Apache Hadoop:** Самая известная реализация (Java). Хранит данные на дисках (HDFS). Надежный, но медленный для итеративных задач.
2. **Apache Spark:** Современный преемник Hadoop. Обрабатывает данные **в оперативной памяти** (in-memory), что в 100 раз быстрее Hadoop для задач машинного обучения.



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №10

**1. Коллективные процессы в MPI. Средства организации коллективных процессов**

* **Теория:**
В операциях участвуют **все** процессы коммуникатора одновременно. Это эффективнее, чем писать циклы из `Send/Recv`.
*Особенности:* Обязательно вызывать во всех процессах.
*Основные функции:*
* `MPI_Bcast` (Broadcast): Один говорит — все слушают (рассылка данных от Master ко всем).
* `MPI_Scatter`: Master делит массив на части и раздает каждому свою часть.
* `MPI_Gather`: Сборка частей от всех процессов обратно в один массив у Master.
* `MPI_Reduce`: Сборка данных с арифметической операцией (сумма, максимум). Например, найти глобальную сумму `local_sum` со всех процессов.


* **Пример (C++ Reduce):**

```cpp
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    // У каждого процесса свое число (например, 10, 20, 30...)
    int my_val = (rank + 1) * 10;
    int global_sum = 0;

    // Складываем my_val со всех процессов и кладем результат в global_sum у процесса 0
    MPI_Reduce(&my_val, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Сумма всех: %d\n", global_sum);
    }

    MPI_Finalize();
    return 0;
}

```

**2. Распределенные хранилища данных**

* **Теория:**
Это файловые системы, где файлы физически размазаны по дискам разных серверов, но пользователь видит их как одну папку.
*Примеры:* **HDFS** (Hadoop Distributed File System), **GFS** (Google File System).
*Принципы:*
1. **Чанки (Blocks):** Файл делится на блоки по 64 МБ или 128 МБ.
2. **Репликация:** Каждый блок сохраняется на 3-х разных серверах. Если один сервер сгорит, данные не пропадут.
3. **Мастер-узел (NameNode):** Хранит "оглавление" — где какой кусок файла лежит.


---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №11

**1. Перезасылка разнотипных данных (MPI). Производные типы**

* **Теория:**
`MPI_Send` умеет отправлять массивы одинаковых типов (только `int` или только `double`). Но что делать, если нужно отправить структуру (`struct`), где есть и `int`, и `char`, и `float`?
Есть два пути:
1. **Упаковка (Packing):** Использовать `MPI_Pack` — вручную сложить все переменные в один буфер байтов, передать его, и распаковать (`MPI_Unpack`). Это медленно.
2. **Производные типы (Derived Datatypes):** Создать свой тип MPI, который описывает структуру.
* `MPI_Type_create_struct` — самый универсальный способ. Мы указываем массивы: длины блоков, смещения (адреса в памяти) и типы полей.
* После создания тип нужно зафиксировать командой `MPI_Type_commit`.


* **Пример (Создание типа):**

```cpp
struct Particle { int id; double x, y; };

// Создаем описание типа для MPI
int block_lengths[2] = {1, 2}; // 1 int и 2 double
MPI_Aint displacements[2];     // Смещения в памяти
MPI_Datatype types[2] = {MPI_INT, MPI_DOUBLE};
MPI_Datatype MPI_PARTICLE_TYPE;

// ... (код вычисления смещений через MPI_Get_address) ...

MPI_Type_create_struct(2, block_lengths, displacements, types, &MPI_PARTICLE_TYPE);
MPI_Type_commit(&MPI_PARTICLE_TYPE);

// Теперь можно слать структуру одной командой
MPI_Send(&my_part, 1, MPI_PARTICLE_TYPE, ...);

```

**2. Коммуникаторы**

* **Теория:**
**Коммуникатор** — это объект, объединяющий группу процессов, которые могут общаться друг с другом.
* `MPI_COMM_WORLD` — стандартный коммуникатор, включающий *все* запущенные процессы.
* Зачем нужны другие? Чтобы изолировать модули программы. Например, процессы 0-3 считают физику, а 4-7 считают графику. Мы можем разбить `MPI_COMM_WORLD` на два под-коммуникатора с помощью `MPI_Comm_split`.
* Внутри нового коммуникатора нумерация (`rank`) начинается снова с 0.



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №12

**1. Отложенные запросы. Тупиковые ситуации (MPI)**

* **Теория:**
* **Отложенные запросы (Non-blocking):** Это операции `MPI_Isend` и `MPI_Irecv`. Буква **I** означает **Immediate**. Функция не ждет завершения передачи, а мгновенно возвращает управление и объект `MPI_Request`. Процесс может делать вычисления, пока данные летят по сети.
* **Тупик (Deadlock):** Ситуация, когда процессы вечно ждут друг друга.
* *Пример:* Процесс 0 делает `Recv` от 1. Процесс 1 делает `Recv` от 0. Никто не делает `Send`. Оба зависли навсегда.
* *Решение:* Следить за порядком вызовов или использовать неблокирующие операции (`Isend`).


* **Пример (Тупик и решение):**

```cpp
// ТУПИК (ОШИБКА)
if (rank == 0) {
    MPI_Recv(..., 1, ...); // Ждет 1
    MPI_Send(..., 1, ...);
} else if (rank == 1) {
    MPI_Recv(..., 0, ...); // Ждет 0
    MPI_Send(..., 0, ...);
}

// РЕШЕНИЕ (Правильный порядок)
if (rank == 0) {
    MPI_Send(..., 1, ...); // Сначала шлем!
    MPI_Recv(..., 1, ...);
} else if (rank == 1) {
    MPI_Recv(..., 0, ...); // Сразу принимаем
    MPI_Send(..., 0, ...);
}

```

**2. Технология OpenMP**

* **Теория:**
OpenMP (Open Multi-Processing) — это API для написания многопоточных приложений на системах с **общей памятью** (многоядерные процессоры).
*Состав:*
1. **Директивы компилятора** (`#pragma omp ...`) — указывают, где параллелить.
2. **Библиотечные функции** (`omp_get_thread_num()`) — управление в runtime.
3. **Переменные окружения** (`OMP_NUM_THREADS`) — настройка без перекомпиляции.
*Принцип:* Модель Fork-Join. Программа начинается как один поток, при встрече `#pragma omp parallel` "разветвляется" (fork) на группу потоков, а в конце параллельного блока сливается обратно (join).

*Основные директивы синхронизации:*
* `#pragma omp barrier`: Все потоки ждут, пока остальные дойдут до этой строки.
* `#pragma omp critical`: Код выполняется строго по одному потоку (защита общих переменных).
* `#pragma omp atomic`: Атомарное обновление переменной (быстрее `critical`, но только для простых операций типа `x++`).

---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №13

**1. Организация асинхронной передачи данных. Обнаружение тупиковых ситуаций и разрешение (MPI)**

* **Теория:**
Асинхронная передача позволяет перекрыть коммуникации вычислениями (скрыть латентность сети).
* **Запуск:** `MPI_Isend` / `MPI_Irecv`. Требуют указатель на `MPI_Request`.
* **Проверка:**
* `MPI_Wait(&request, ...)` — блокирует процесс, пока передача, связанная с этим запросом, не завершится.
* `MPI_Test(&request, &flag, ...)` — проверяет, завершилась ли передача, но не блокирует. Возвращает `flag=true/false`.


* **Разрешение тупиков:** Использование асинхронности — лучший способ избежать Deadlock. Если мы используем `MPI_Isend`, программа не виснет, а идет дальше к `MPI_Recv`.



**2. Коммуникаторы**

* **Теория:**
**Коммуникатор** — это объект, объединяющий группу процессов, которые могут общаться друг с другом.
* `MPI_COMM_WORLD` — стандартный коммуникатор, включающий *все* запущенные процессы.
* Зачем нужны другие? Чтобы изолировать модули программы. Например, процессы 0-3 считают физику, а 4-7 считают графику. Мы можем разбить `MPI_COMM_WORLD` на два под-коммуникатора с помощью `MPI_Comm_split`.
* Внутри нового коммуникатора нумерация (`rank`) начинается снова с 0.

---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №14

**1. Асинхронная передача данных (без блокировки)**

* **Теория:**
Основное отличие от блокирующей передачи: мы **не имеем права трогать буфер передачи**, пока операция не завершена.
Если мы сделали `MPI_Isend(&buffer, ...)` и тут же изменили `buffer[0] = 5`, мы не знаем, что ушло в сеть — старое значение или новое (состояние гонки). Нужно дождаться `MPI_Wait`.
* **Пример (Асинхронность):**

```cpp
int buf = 100;
MPI_Request req;

// Начинаем отправку, функция вернет управление мгновенно
MPI_Isend(&buf, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &req);

// В ЭТО ВРЕМЯ можно делать полезную работу!
printf("Я считаю математику, пока данные уходят...\n");

// Теперь обязательно убеждаемся, что данные ушли, перед тем как менять buf
MPI_Wait(&req, MPI_STATUS_IGNORE);

```

**2. Примеры распределенных хранилищ данных**

* **Теория:**
* **HDFS (Hadoop Distributed File System):** Самая популярная. Оптимизирована для чтения огромных файлов (write-once-read-many). Мастер-узел (NameNode) хранит метаданные, рабочие узлы (DataNodes) хранят блоки данных.
* **Lustre:** Высокопроизводительная параллельная ФС, используется на суперкомпьютерах. Очень быстрая.
* **Amazon S3 (Object Storage):** Данные хранятся не как файлы в папках, а как объекты с ID. Доступ через HTTP API.
* **Ceph:** Универсальная система, может работать и как блочное устройство, и как файловая система, и как объектное хранилище.



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №15

**1. Программирование обмена сообщениями с измерением временных параметров (MPI)**

* **Теория:**
Для замеров времени в MPI используется функция `MPI_Wtime()`. Она возвращает время в секундах (тип `double`) от какого-то момента в прошлом.
* Точность таймера зависит от железа, можно узнать через `MPI_Wtick()`.
* Важно: часы на разных узлах кластера могут быть не синхронизированы! Нельзя сравнивать `Wtime` процесса 0 и процесса 1 напрямую. Обычно замеряют интервал внутри *одного* процесса.
* *Ping-Pong тест:* Процесс А шлет Б, Б шлет обратно А. Время делим на 2 — получаем латентность.


* **Пример:**

```cpp
double start_time, end_time;

// Синхронизация перед замером, чтобы все начали одновременно
MPI_Barrier(MPI_COMM_WORLD); 

start_time = MPI_Wtime();

// ... какой-то долгий обмен данными или вычисления ...
MPI_Send(...);

end_time = MPI_Wtime();

printf("Время операции: %f секунд\n", end_time - start_time);

```

**2. Директивы OpenMP**

* **Теория:**
*Основные директивы синхронизации:*
* `#pragma omp barrier`: Все потоки ждут, пока остальные дойдут до этой строки.
* `#pragma omp critical`: Код выполняется строго по одному потоку (защита общих переменных).
* `#pragma omp atomic`: Атомарное обновление переменной (быстрее `critical`, но только для простых операций типа `x++`).
* `#pragma omp master`: Блок выполняет только главный поток (master, поток №0), остальные пропускают.
* `#pragma omp ordered`: Заставляет участок внутри цикла выполняться в строгом порядке итераций.

*Директивы распределения работы:*
* `#pragma omp parallel`: Создает параллельную область (fork-join).
* `#pragma omp for`: Делит итерации цикла между потоками.
* `#pragma omp sections`: Для разных независимых задач (функциональный параллелизм).

---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №16

**1. Прием сообщений. Встроенные переменные MPI**

* **Теория:**
* **Прием:** Функция `MPI_Recv`. Она блокирует процесс, пока сообщение не придет.
* **Аргументы:** `(буфер, кол-во, тип, от_кого, тэг, коммуникатор, статус)`.
* **Джокеры (Wildcards):** Иногда мы не знаем, от кого придет сообщение или с каким тэгом.
* `MPI_ANY_SOURCE` — принять от кого угодно.
* `MPI_ANY_TAG` — принять с любым тэгом.


* **Структура `MPI_Status`:** Если использовали джокеры, то узнать, кто реально прислал данные, можно через объект `status` (поля `status.MPI_SOURCE` и `status.MPI_TAG`).


* **Пример:**

```cpp
MPI_Status status;
int buf[10];
// Ждем сообщение от ЛЮБОГО процесса с тэгом 0
MPI_Recv(buf, 10, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);

printf("Получено сообщение от процесса %d\n", status.MPI_SOURCE);

```

**2. Синхронизация в OpenMP**

* **Теория:**
Потоки часто работают с разной скоростью. Чтобы они не мешали друг другу:
1. `#pragma omp barrier` — все ждут всех.
2. `#pragma omp critical` — только один поток за раз (как мьютекс).
3. `#pragma omp atomic` — атомарная операция (быстрее critical, для `x++`, `x+=y`).
4. `#pragma omp ordered` — заставляет часть цикла выполняться строго по порядку итераций (как в последовательном коде).



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №17

**1. Средства передачи данных с буферизацией (MPI)**

* **Теория:**
Стандартный `MPI_Send` может заблокироваться, если у получателя нет места. Чтобы избежать ожидания, используется **буферизованная отправка** — `MPI_Bsend`.
* **Как работает:** Пользователь выделяет кусок памяти (`MPI_Buffer_attach`). При вызове `MPI_Bsend` MPI копирует сообщение в этот буфер и сразу возвращает управление программе (функция завершается). Потом MPI сам тихонько отправит данные получателю.
* **Плюс:** Никогда не вызывает Deadlock.
* **Минус:** Нужно тратить память и время на копирование. Если буфер переполнится — ошибка программы.


* **Пример:**

```cpp
int buffer_size = 1000 + MPI_BSEND_OVERHEAD;
char* buffer = malloc(buffer_size);
MPI_Buffer_attach(buffer, buffer_size); // Даем MPI память

// Bsend копирует данные в buffer и сразу идет дальше
MPI_Bsend(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);

MPI_Buffer_detach(&buffer, &buffer_size); // Забираем память обратно

```

**2. Дополнительные переменные среды и функции (OpenMP)**

* **Теория:**
Управлять поведением OpenMP можно без перекомпиляции кода.
* **Переменные среды (в консоли):**
* `OMP_NUM_THREADS=4` — задать число потоков.
* `OMP_SCHEDULE="dynamic"` — выбрать тип планирования циклов.
* `OMP_NESTED=TRUE` — разрешить вложенный параллелизм (параллельность внутри параллельности).


* **Функции (в коде):**
* `omp_set_num_threads(int)` — то же, что переменная среды, но внутри кода.
* `omp_get_wtime()` — таймер (аналог MPI_Wtime).
* `omp_get_max_threads()` — сколько потоков доступно максимум.

---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №18

**1. Процедуры обмена MPI сообщениями. Типы данных, структура сообщения**

* **Теория:**
Сообщение в MPI похоже на физическое письмо ("конверт").
Оно состоит из **данных** (тело письма) и **атрибутов** (надписи на конверте).
* **Структура вызова:** `MPI_Send(buf, count, datatype, dest, tag, comm)`.


1. `buf`: Адрес начала данных.
2. `count`: Количество элементов.
3. `datatype`: Базовые типы MPI (`MPI_INT`, `MPI_DOUBLE`, `MPI_CHAR`...) или созданные пользователем. Важно: типы должны совпадать на отправке и приеме.
4. `dest`: Ранг получателя.
5. `tag`: Целое число (метка) для различения сообщений (например, 1 — данные, 2 — команда "стоп").
6. `comm`: Коммуникатор (группа процессов).



**2. Параллельные вычислительные системы, примеры**

* **Теория:**
Примеры реальных систем:
1. **Кластеры Beowulf:** Собраны из обычных ПК, соединенных сетью Ethernet. Дешево, сердито.
2. **Мэйнфреймы (IBM zSeries):** Огромные надежные машины для банков. Общая память.
3. **Суперкомпьютеры (Cray, Summit, Fugaku):** Тысячи спецпроцессоров, соединенных проприетарной шиной (Interconnect). Используются для моделирования ядерных взрывов, погоды.
4. **GPU-фермы:** Серверы с картами NVIDIA Tesla/A100. Для ИИ и рендеринга.



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №19

**1. Общие функции MPI. Инициализация многопроцессорной системы**

* **Теория:**
Минимальный набор ("джентльменский набор") любой MPI программы — это 6 функций.
1. `MPI_Init(&argc, &argv)` — Запускает среду MPI. До этого вызова никакие функции MPI не работают.
2. `MPI_Finalize()` — Очищает ресурсы. После этого MPI не работает.
3. `MPI_Comm_size(...)` — Узнать общее число процессов.
4. `MPI_Comm_rank(...)` — Узнать свой номер.
5. `MPI_Send` — Отправить.
6. `MPI_Recv` — Принять.



**2. Использование OpenMP**

* **Теория:**
Когда использовать?
* Когда есть **общая память** (один компьютер с многоядерным CPU).
* Когда нужно распараллелить существующий последовательный код **постепенно** (добавляя `#pragma` к циклам `for`).
* Когда задача хорошо делится на независимые итерации (Data Parallelism).
* *Не подходит* для кластеров (там MPI).



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №20

**1. Технология распараллеливания MPI. ПО. Запуск. Обязательные процедуры**

* **Теория:**
* **ПО:** Для работы нужна библиотека MPI (реализации: MPICH, OpenMPI, Intel MPI).
* **Компилятор:** Обычно это скрипт-обертка: `mpicc` (для C) или `mpicxx` (для C++). Он сам подключает нужные библиотеки.
* Пример: `mpicc my_prog.c -o my_prog`


* **Запуск:** Программа запускается не напрямую, а через загрузчик `mpiexec` или `mpirun`. Он размножает процесс на N копий.
* Пример: `mpirun -np 4 ./my_prog` (запустить 4 копии).


* **Обязательные процедуры:**
  * `MPI_Init(&argc, &argv)` — Запускает среду MPI. До этого вызова никакие функции MPI не работают.
  * `MPI_Finalize()` — Очищает ресурсы. После этого MPI не работает.



**2. Модель данных OpenMP**

* **Теория:**
В OpenMP память делится на два типа:
1. **Shared (Общая):** Переменная одна на всех. Все потоки видят и меняют одну ячейку памяти. (По умолчанию все переменные, объявленные *до* параллельной области — shared).
2. **Private (Частная):** У каждого потока создается своя личная копия переменной.


* **Клаузы управления данными:**
* `private(x)`: Создать копию `x` для каждого потока (не инициализирована).
* `firstprivate(x)`: Создать копию и скопировать значение из главной переменной.
* `shared(x)`: Явно сказать, что переменная общая.




* **Пример:**

```cpp
int x = 5; // Shared
#pragma omp parallel private(x)
{
    x = 1; // Меняет только свою локальную копию
}
// Здесь x все еще равен 5

```
Финишная прямая! Вот ответы на оставшиеся **билеты №21 — №25**.

Многие вопросы здесь пересекаются с предыдущими (повторение — мать учения), но я дам краткие, емкие ответы, чтобы у тебя была полная картина по каждому билету.

---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №21

**1. Директивы OpenMP. Директивы синхронизации и взаимодействия потоков во времени**

* **Теория:**
Потоки выполняются независимо, и без контроля могут возникнуть "гонки данных".
* `#pragma omp barrier`: Все потоки ждут, пока остальные дойдут до этой строки.
* `#pragma omp critical`: Код выполняется строго по одному потоку (защита общих переменных).
* `#pragma omp atomic`: Атомарное обновление переменной (быстрее `critical`, но только для простых операций типа `x++`).
* `#pragma omp ordered`: Заставляет участок внутри цикла выполняться в строгом порядке итераций (как если бы параллелизма не было).



**2. Коммуникаторы (MPI)**

* **Теория:**
* **Коммуникатор** — это группа процессов плюс контекст связи.
* **MPI_COMM_WORLD** — базовый коммуникатор (все процессы).
* Для создания подгрупп используют `MPI_Comm_split(old_comm, color, key, &new_comm)`. Процессы с одинаковым `color` попадут в один новый коммуникатор.
* *Зачем:* Чтобы ограничить область действия коллективных операций (например, сделать `Barrier` только для половины процессов).



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №22

**1. Директивы OpenMP. Директивы SECTIONS, SECTION, DO, ENDDO. Ключи организации цикла**

* **Теория:**
Это директивы распределения работы (Work-sharing constructs). Они не создают новые потоки, а делят работу между уже существующими.
* **Циклы (DO / for):**
* Fortran: `!$OMP DO` ... `!$OMP END DO`
* C++: `#pragma omp for`. Делит итерации цикла между потоками.


* **Секции (SECTIONS):** Используются, когда есть несколько *разных* независимых задач (функциональный параллелизм). Например, один поток читает файл, второй считает, третий рисует.
* C++: `#pragma omp sections` { `#pragma omp section` { ... } }




* **Пример (Sections):**
```cpp
#pragma omp parallel sections
{
    #pragma omp section
    { printf("Я делаю задачу А\n"); }

    #pragma omp section
    { printf("Я делаю задачу Б (совсем другую)\n"); }
}

```



**2. Распределенные хранилища данных**

* **Теория:**
Это файловые системы, где файлы физически размазаны по дискам разных серверов, но пользователь видит их как одну папку.
*Примеры:* **HDFS** (Hadoop Distributed File System), **GFS** (Google File System), **Lustre**.
*Принципы:*
1. **Чанки (Blocks):** Файл делится на блоки по 64 МБ или 128 МБ.
2. **Репликация:** Каждый блок сохраняется на 3-х разных серверах. Если один сервер сгорит, данные не пропадут.
3. **Мастер-узел (NameNode):** Хранит "оглавление" — где какой кусок файла лежит.

---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №23

**1. Директивы OpenMP. Запуск, компиляция. Определение области. Переменные окружения. Встроенные функции. Управление памятью**

* **Теория:**
* **Запуск:** Обычный запуск исполняемого файла (`./app`). Число потоков берется из окружения.
* **Компиляция:** Ключ `-fopenmp` (GCC) или `/openmp` (MSVC).
* **Параллельная область:** Блок `#pragma omp parallel { ... }`. Вне его работает только один мастер-поток.
* **Переменные окружения:** `OMP_NUM_THREADS` (число потоков), `OMP_STACKSIZE` (размер стека потока).
* **Встроенные функции:** `omp_get_thread_num()`, `omp_get_num_threads()`.
* **Управление памятью:** Клаузы `shared(...)` (общая), `private(...)` (личная, мусор на входе), `firstprivate(...)` (личная, копия значения извне).



**2. Коллективные процессы в MPI**

* **Теория:**
В операциях участвуют **все** процессы коммуникатора одновременно. Это эффективнее, чем писать циклы из `Send/Recv`.
*Особенности:* Обязательно вызывать во всех процессах.
*Основные функции:*
* `MPI_Bcast` (Broadcast): Один говорит — все слушают (рассылка данных от Master ко всем).
* `MPI_Scatter`: Master делит массив на части и раздает каждому свою часть.
* `MPI_Gather`: Сборка частей от всех процессов обратно в один массив у Master.
* `MPI_Reduce`: Сборка данных с арифметической операцией (сумма, максимум). Например, найти глобальную сумму `local_sum` со всех процессов.

---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №24

**1. Обзор средств программирования. Система и стандарты OpenMP**

* **Теория:**
OpenMP (Open Multi-Processing) — это API для написания многопоточных приложений на системах с **общей памятью** (многоядерные процессоры).
*Состав:*
1. **Директивы компилятора** (`#pragma omp ...`) — указывают, где параллелить.
2. **Библиотечные функции** (`omp_get_thread_num()`) — управление в runtime.
3. **Переменные окружения** (`OMP_NUM_THREADS`) — настройка без перекомпиляции.
*Принцип:* Модель Fork-Join. Программа начинается как один поток, при встрече `#pragma omp parallel` "разветвляется" (fork) на группу потоков, а в конце параллельного блока сливается обратно (join). Позволяет распараллеливать код постепенно, не переписывая всю программу.

**2. Использование встроенных функций MPI для измерения временных характеристик системы**

* **Теория:**
* Основная функция: `MPI_Wtime()`.
* Возвращает астрономическое время в секундах (double).
* Для замера интервала: `start = MPI_Wtime(); ... end = MPI_Wtime(); result = end - start;`.
* `MPI_Wtick()` — показывает точность таймера (разрешающую способность).



---

### ЭКЗАМЕНАЦИОННЫЙ БИЛЕТ №25

**1. Большие задачи. Вычислительная техника и технологии для больших задач**

* **Теория:**
"Большие задачи" (Grand Challenges) — это задачи, требующие колоссальных вычислительных ресурсов, которые невозможно решить на обычном ПК за разумное время.
*Примеры:* Прогноз погоды/климата, моделирование сворачивания белков, ядерные реакции, анализ Big Data, генетика, астрофизика.
*Технологии:*
* **Суперкомпьютеры:** (TOP500) — тысячи узлов, соединенных быстрой сетью (InfiniBand).
* **Грид-системы (Grid):** Объединение географически распределенных компьютеров (например, через интернет) для одной задачи.
* **Облачные вычисления (Cloud):** Аренда мощностей (AWS, Azure) по требованию.
* **MPI:** для связи тысяч узлов.
* **MapReduce/Spark:** для обработки петабайтов данных.

**2. Передача и прием сообщений. Встроенные переменные MPI**

* **Теория:**
Сообщение в MPI состоит из **данных** (тело письма) и **атрибутов** (надписи на конверте).
* **Структура вызова:** `MPI_Send(buf, count, datatype, dest, tag, comm)`.
* **Прием:** Функция `MPI_Recv`. Она блокирует процесс, пока сообщение не придет.
* Функции: `MPI_Send` (отправка), `MPI_Recv` (прием).
* **Встроенные константы/переменные:**
* `MPI_COMM_WORLD`: Главный коммуникатор.
* `MPI_STATUS_IGNORE`: Если нам не нужен статус при приеме.
* `MPI_ANY_SOURCE`: Принять от кого угодно.
* `MPI_ANY_TAG`: Принять с любым тегом.
* Типы данных: `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`, `MPI_CHAR`.